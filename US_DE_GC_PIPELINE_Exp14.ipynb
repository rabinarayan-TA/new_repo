{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2PrUHJY--fw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c062f62c-85b4-4270-9e81-878eab651fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyarrow.parquet as pq\n",
        "from datetime import timedelta as td\n",
        "drive.mount('/content/drive')\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eFgUF4Ojntv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73fb5597-830a-4889-f2da-fb267d6c1bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas_flavor\n",
            "  Downloading pandas_flavor-0.5.0-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.9/dist-packages (from pandas_flavor) (0.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.9/dist-packages (from pandas_flavor) (2022.12.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.9/dist-packages (from pandas_flavor) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23->pandas_flavor) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23->pandas_flavor) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23->pandas_flavor) (1.22.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from xarray->pandas_flavor) (23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas_flavor) (1.16.0)\n",
            "Installing collected packages: pandas_flavor\n",
            "Successfully installed pandas_flavor-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xgboost==1.6\n",
            "  Downloading xgboost-1.6.0-py3-none-manylinux2014_x86_64.whl (193.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from xgboost==1.6) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from xgboost==1.6) (1.22.4)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 1.7.4\n",
            "    Uninstalling xgboost-1.7.4:\n",
            "      Successfully uninstalled xgboost-1.7.4\n",
            "Successfully installed xgboost-1.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_flavor\n",
        "!pip install xgboost==1.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaIDjP24_ySH"
      },
      "source": [
        "### Defining Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im-IStt5_2HL"
      },
      "outputs": [],
      "source": [
        "coops_top11 = [\n",
        "'kansas city nebraska sioux city omaha',\n",
        "'sacramento fresno reno bakersfield',\n",
        "'washington dc baltimore eastern shore',\n",
        "'dallas tyler',\n",
        "'denver col springs s colorado',\n",
        "'triad wilmington sw va wi s gnw',\n",
        "'albuquerque el paso',\n",
        "'atlanta nw georgia',\n",
        "'pittsburgh johnstown erie',\n",
        "'philadelphia',\n",
        "'st louis',\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "all_coops=['denver col springs s colorado',\n",
        " 'new england',\n",
        " 'terrehaute evansville cape gir paducah',\n",
        " 'st louis',\n",
        " 'philadelphia',\n",
        " 'tampa st pete orlando',\n",
        " 'pittsburgh johnstown erie',\n",
        " 'portland medford eugene bend',\n",
        " 'memphis mississippi',\n",
        " 'south la',\n",
        " 'miami w palm beach ft myers',\n",
        " 'kansas city nebraska sioux city omaha',\n",
        " 'washington dc baltimore eastern shore',\n",
        " 'salt lake city s wyoming',\n",
        " 'chicago',\n",
        " 'atlanta nw georgia',\n",
        " 'triad wilmington sw va wi s gnw',\n",
        " 'greater indiana',\n",
        " 'albuquerque el paso',\n",
        " 'dallas tyler',\n",
        " 'sacramento fresno reno bakersfield',\n",
        " 'san francisco santa barbara',\n",
        " 'little rock shreveport monroe',\n",
        " 'wichita topeka ok city wichita falls',\n",
        " 'seattle tacoma',\n",
        " 'sw north carolina e south carolina',\n",
        " 'tulsa fort smith',\n",
        " 'savannah augusta macon albany columbus',\n",
        " 'san antonio austin waco victoria',\n",
        " 'w virginia knoxville bristol kingsport',\n",
        " 'wisconsin rockford',\n",
        " 'richmond shenandoah valley norfolk',\n",
        " 'lexington louisville',\n",
        " 'houston beaumont',\n",
        " 'valley corpus christi laredo',\n",
        " 'susquehanna',\n",
        " 'jacksonville tallahassee',\n",
        " 'cleveland',\n",
        " 'michigan',\n",
        " 'des moines quad cities peoria cent il',\n",
        " 'alabama nw florida',\n",
        " 'alaska',\n",
        " 'montana',\n",
        " 'new york metro',\n",
        " 'columbus cincy dayton toledo lima',\n",
        " 'minneapolis st paul rochester duluth',\n",
        " 'joplin quincy springfield col jeff city',\n",
        " 'hawaii',\n",
        " 'albany buffalo rochester syracuse',\n",
        " 'greater tennessee valley',\n",
        " 'n dakota s dakota w minnesota',\n",
        " 'los angeles san diego',\n",
        " 'e washington nw idaho ne oregon',\n",
        " 'boise twin falls idaho falls',\n",
        " 'phoenix las vegas tucson yuma el centro',\n",
        " 'amarillo lubbock']\n",
        "s3=False\n",
        "LTO_ITEMS =[2930,1751,2931,\n",
        "1270,\n",
        "6244,\n",
        "118,\n",
        "557,\n",
        "14962,\n",
        "657,\n",
        "14938,\n",
        "14970,\n",
        "2190,\n",
        "5909,\n",
        "3573,\n",
        "1120,\n",
        "1921,\n",
        "2157,\n",
        "2108,\n",
        "428,\n",
        "2133,\n",
        "3783,\n",
        "14960,\n",
        "14924,\n",
        "2936,\n",
        "2156,\n",
        "2100,\n",
        "427,\n",
        "9727,\n",
        "2127,\n",
        "3780,\n",
        "14959,\n",
        "14889,\n",
        "14888,\n",
        "10,\n",
        "8971,\n",
        "4257,\n",
        "14925,\n",
        "601,\n",
        "5910,\n",
        "1926,\n",
        "6260,\n",
        "2088,\n",
        "424,\n",
        "4055,\n",
        "3779,\n",
        "14958,\n",
        "14926,\n",
        "14929,\n",
        "14869,\n",
        "558,\n",
        "126,\n",
        "782,\n",
        "1014,\n",
        "1000,\n",
        "1012,\n",
        "457,\n",
        "258,\n",
        "462,\n",
        "485,\n",
        "1975,\n",
        "1121,\n",
        "6763\n",
        "]\n",
        "start_date = \"2019-01-01\"\n",
        "end_date = \"2023-01-27\"\n",
        "exclusion_dates = ['2020-03-15', '2020-03-16', '2020-03-17', '2020-03-18', \n",
        "                   '2020-03-19', '2020-03-20', '2020-03-21', '2020-03-22', \n",
        "                   '2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26', \n",
        "                   '2020-03-27', '2020-03-28', '2020-03-29', '2020-03-30', \n",
        "                   '2020-03-31', '2020-04-01', '2020-04-02', '2020-04-03', \n",
        "                   '2020-04-04', '2020-04-05', '2020-04-06', '2020-04-07', \n",
        "                   '2020-04-08', '2020-04-09', '2020-04-10', '2020-04-11', \n",
        "                   '2020-04-12', '2020-04-13', '2020-04-14', '2020-04-15', \n",
        "                   '2020-04-16', '2020-04-17', '2020-04-18', '2020-04-19', \n",
        "                   '2020-04-20', '2020-04-21', '2020-04-22', '2020-04-23', \n",
        "                   '2020-04-24', '2020-04-25', '2020-04-26', '2020-04-27', \n",
        "                   '2020-04-28', '2020-04-29', '2020-04-30', '2020-05-01', \n",
        "                   '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', \n",
        "                   '2020-05-06', '2020-05-07', '2020-05-08', '2020-05-09', \n",
        "                   '2020-05-10', '2020-05-11', '2020-05-12', '2020-05-13', \n",
        "                   '2020-05-14', '2020-05-15', '2020-05-16', '2020-05-17', \n",
        "                   '2020-05-18', '2020-05-19', '2020-05-20', '2020-05-21', \n",
        "                   '2020-05-22', '2020-05-23', '2020-05-24', '2020-05-25', \n",
        "                   '2020-05-26', '2020-05-27', '2020-05-28', '2020-05-29', \n",
        "                   '2020-05-30', '2020-05-31', '2020-06-01', '2020-06-02', \n",
        "                   '2020-06-03', '2020-06-04', '2020-06-05', '2020-06-06', \n",
        "                   '2020-06-07', '2020-06-08', '2020-06-09', '2020-06-10', \n",
        "                   '2020-06-11', '2020-06-12', '2020-06-13', '2020-06-14', \n",
        "                   '2020-06-15', '2020-06-16', '2020-06-17', '2020-06-18', \n",
        "                   '2020-06-19', '2020-06-20', '2020-06-21', '2020-06-22', \n",
        "                   '2020-06-23', '2020-06-24', '2020-06-25', '2020-06-26', \n",
        "                   '2020-06-27', '2020-06-28', '2020-06-29', '2020-06-30',\n",
        "                      '2022-07-04','2022-07-11','2022-07-18','2022-07-25']\n",
        "\n",
        "DELISTED_ITEMS = [4775, 14932, 14885]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vudw7Mdn_Rt9"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBHt1rCm_LRF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBRegressor\n",
        "#from ta_lib.usa.modelling import gc_model as gcm\n",
        "import shutil\n",
        "#from ta_lib.usa import model_constants\n",
        "\n",
        "\n",
        "import time \n",
        "import warnings \n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import pandas_flavor as pf\n",
        "\n",
        "\n",
        "@pf.register_dataframe_method\n",
        "def pd_cols_to_datetime(df, col_='pos_busn_dt',dt_format=None):\n",
        "    '''Convert a column in pandas dataframe to datetime.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: pd.Dataframe\n",
        "    col_ : str, default is \"pos_busn_dt\"\n",
        "    dt_format : str, default is None\n",
        "        Same as the format parameter in pd.to_datetimeb \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df : pd.Dataframe\n",
        "    '''\n",
        "\n",
        "    if dt_format is None:\n",
        "        df[col_] = pd.to_datetime(df[col_])\n",
        "    else:\n",
        "        df[col_] = pd.to_datetime(df[col_],format=dt_format)\n",
        "    return df\n",
        "@pf.register_dataframe_method\n",
        "def get_week_start_dt(df, date_col_name='pos_busn_dt'):\n",
        "    '''Get week start date for date columns columns in pandas dataframe to datetime.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: pd.Dataframe\n",
        "    date_col_name : str, default is \"pos_busn_dt\"\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df : adds `week_st_dt` column to dataframe\n",
        "    '''\n",
        "\n",
        "    df['week_st_dt'] = df[date_col_name].apply(lambda x: x - td(days=x.weekday()))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdBqfTM0-2h8"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3JzG5fq-2UP"
      },
      "outputs": [],
      "source": [
        "cfg_refit_model =False\n",
        "generate_training_dataset=False\n",
        "save_gc_predictions = False\n",
        "save_gc_elasticitity=False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling for category"
      ],
      "metadata": {
        "id": "68BQnO6ZDxV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def scaling_for_category(df_demand_predictions,ITEM_MASTER,pred_col):\n",
        "\n",
        "  \n",
        "  df_item_master = pd.read_csv(ITEM_MASTER).rename(columns={'sld_menu_itm_id':'dv_id'})\n",
        "  df_demand_predictions  = df_demand_predictions.merge(df_item_master[['item_category','dv_id']],on=['dv_id'],how='left')\n",
        "  str_count_path = os.path.join(ONE_DATA_ROOT,'y_data','04_base_price_asp','raw','coop=')\n",
        "  print(str_count_path)\n",
        "  df_train=pd.DataFrame()\n",
        "\n",
        "  for coop in tqdm(coops_top11):\n",
        "\n",
        "    #for train \n",
        "    str_cnt_df = pd.read_parquet(str_count_path+coop).rename(columns={'sld_menu_itm_id':'dv_id','week_st_dt':'pos_busn_dt'}).assign(coop_name=coop)[['dv_id','pos_busn_dt','store_count','coop_name']]\n",
        "    str_cnt_df = str_cnt_df.merge(df_item_master[['item_category','dv_id']],on=['dv_id'],how='left')\n",
        "    str_cnt_df['pos_busn_dt']=pd.to_datetime(str_cnt_df['pos_busn_dt'])\n",
        "    \n",
        "    str_cnt_grouped = str_cnt_df.groupby(['coop_name','pos_busn_dt','item_category'])['store_count'].max().reset_index()\n",
        "    str_cnt_grouped['pos_busn_dt']=pd.to_datetime(str_cnt_grouped['pos_busn_dt'])\n",
        "    \n",
        "    df_demand_coop_train = df_demand_predictions.query('coop_name==@coop and split==\"train\"')\n",
        "    df_demand_coop_train =df_demand_coop_train.merge(str_cnt_df[['coop_name','pos_busn_dt','dv_id','store_count']],on=['coop_name','pos_busn_dt','dv_id'],how='left')\n",
        "    df_demand_coop_train[pred_col] = df_demand_coop_train[pred_col]*df_demand_coop_train['store_count']\n",
        "    df_demand_coop_train.drop(columns=['store_count'],inplace=True)\n",
        "\n",
        "    df_demand_coop_train= df_demand_coop_train.merge(str_cnt_grouped,on=['coop_name','pos_busn_dt','item_category'],how='left')\n",
        "    df_demand_coop_train[pred_col] =df_demand_coop_train[pred_col]/df_demand_coop_train['store_count']\n",
        "\n",
        "    df_demand_coop_train.drop(columns=['store_count','item_category'],inplace=True)\n",
        "\n",
        "\n",
        "    #for test \n",
        "    str_cnt_df_dv_id=str_cnt_df.copy()\n",
        "    str_cnt_df_category=str_cnt_df.copy()\n",
        "    \n",
        "\n",
        "    df_demand_coop_test = df_demand_predictions.query('coop_name==@coop and split==\"test\"')\n",
        "    str_cnt_df_dv_id['rank']=str_cnt_df_dv_id.groupby([\"coop_name\",\"dv_id\"])[\"pos_busn_dt\"].rank(method=\"first\", ascending=False)\n",
        "\n",
        "    str_cnt_df_dv_id =str_cnt_df_dv_id.query('rank==1.0')\n",
        "\n",
        "    df_demand_coop_test =df_demand_coop_test.merge(str_cnt_df_dv_id[['coop_name','dv_id','store_count']],on=['coop_name','dv_id'],how='left')\n",
        "    df_demand_coop_test[pred_col] = df_demand_coop_test[pred_col]*df_demand_coop_test['store_count']\n",
        "    df_demand_coop_test.drop(columns=['store_count'],inplace=True)\n",
        "    \n",
        "    str_cnt_df_category['rank'] = str_cnt_df_category.groupby([\"coop_name\",\"item_category\"])[\"pos_busn_dt\"].rank(method=\"first\", ascending=False)\n",
        "    str_cnt_df_category =str_cnt_df_category.query('rank==1.0')\n",
        "    str_cnt_df_category = str_cnt_df_category[['coop_name','item_category','store_count']]\n",
        "\n",
        "    df_demand_coop_test= df_demand_coop_test.merge(str_cnt_df_category,on=['coop_name','item_category'],how='left')\n",
        "    df_demand_coop_test[pred_col] =df_demand_coop_test[pred_col]/df_demand_coop_test['store_count']\n",
        "    df_demand_coop_test.drop(columns=['store_count','item_category'],inplace=True)\n",
        "\n",
        "    df_demand_coop_train=df_demand_coop_train.append(df_demand_coop_test,ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "    df_train=df_train.append(df_demand_coop_train,ignore_index=True)\n",
        "\n",
        "  return df_train.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "l2XjhF97Dw1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_top_items(item_master_org_path,offer_path,item_mapping,item_master_scaling_path):\n",
        "\n",
        "  item_master_org=pd.read_excel(item_master_org_path,sheet_name='Item_Master').query('LTO_Flag==0')[['instore transaction','sld_menu_itm_id','instore units','instore net sales']]\n",
        "  \n",
        "  item_master_org['sld_menu_itm_id']=item_master_org['sld_menu_itm_id'].astype(np.int64).astype(str)\n",
        "  ## sp sp mapping \n",
        "  df_item_grp = pd.read_csv(item_mapping)\n",
        "  df_item_grp['sld_menu_itm_id']=df_item_grp['sld_menu_itm_id'].astype(np.int64).astype(str)\n",
        "  item_master_org = item_master_org.merge(df_item_grp,on = ['sld_menu_itm_id'],how = 'left')\n",
        "  fil_ = item_master_org.group_id.isnull()\n",
        "  item_master_org.loc[fil_, \"group_id\"] = item_master_org.loc[fil_, \"sld_menu_itm_id\"]\n",
        "  item_master_org['group_id']=item_master_org['group_id'].astype(np.int64).astype(str)\n",
        "\n",
        "  item_master_org.drop(columns=['sld_menu_itm_id'],inplace=True)\n",
        "  item_master_org.rename(columns={'group_id':'sld_menu_itm_id'},inplace=True)\n",
        "\n",
        "  ## item master scaling maping \n",
        "\n",
        "  item_master_scaling=pd.read_parquet(item_master_scaling_path).dropna(subset=['Base_item_ID'])\n",
        "  item_master_scaling['Base_item_ID']=item_master_scaling['Base_item_ID'].astype(np.int64).astype(str)\n",
        "  item_master_scaling['sld_menu_itm_id']=item_master_scaling['sld_menu_itm_id'].astype(np.int64).astype(str)\n",
        "  item_master_org=item_master_org.merge(item_master_scaling,on=['sld_menu_itm_id'],how='left')\n",
        "  item_master_org['sld_menu_itm_id']= np.where(item_master_org['Base_item_ID'].isna(),item_master_org['sld_menu_itm_id'],item_master_org['Base_item_ID'])\n",
        "  item_master_org.drop(columns=['Scaling_Factor','Base_item_ID'],inplace=True)\n",
        " \n",
        "\n",
        "  def return_top_100(df,col):\n",
        "      df = df.dropna(subset=[col,'sld_menu_itm_id'])\n",
        "      df['rank']=df[col].rank(method=\"first\", ascending=False)\n",
        "      df = df.sort_values('rank')\n",
        "      return list(set(df.head(100)['sld_menu_itm_id']))\n",
        "  final_items=[]\n",
        "  \n",
        "  selection_cols = ['instore transaction','instore units','instore net sales']\n",
        "  for sol in selection_cols:   \n",
        "      final_items.extend(return_top_100(item_master_org,sol))\n",
        "      print(final_items)\n",
        "  \n",
        "  final_items=list(map(int,final_items))\n",
        " \n",
        "  offer_item_freq= dict()\n",
        "  root_dir = offer_path\n",
        "  for fil in os.listdir(root_dir):\n",
        "      offers=[col.split('_')[2] for col in pd.read_parquet(os.path.join(root_dir,fil)).columns if col.startswith('dd_')]\n",
        "      for offer in offers:\n",
        "          if offer!='off':\n",
        "              temp_items = list(map(int,offer.replace('|','-').split('-')))\n",
        "              for item in temp_items:\n",
        "                  if item not in offer_item_freq:\n",
        "                    if item not in LTO_ITEMS:\n",
        "                      offer_item_freq[item]=1\n",
        "                  else:\n",
        "                    if item not in LTO_ITEMS:\n",
        "                      offer_item_freq[item]=offer_item_freq[item]+1\n",
        "                      \n",
        "   \n",
        "  top_100_items  = set(list(dict(sorted(offer_item_freq.items(), key=lambda item: item[1],reverse=True)[:100]).keys()))\n",
        "  item_select = set(final_items).union(top_100_items)\n",
        "  item_select=list(map(int,item_select))\n",
        "  return item_select\n",
        "      "
      ],
      "metadata": {
        "id": "-hd940iGvnZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_3IWukcBZbj"
      },
      "source": [
        "### utility function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs_E77OwBcJt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _get_feature_cat_mapper(item_master_path):\n",
        "    item_master = pd.read_csv(item_master_path)\n",
        "    item_master[\"feature_name\"] = item_master[\"sld_menu_itm_id\"].apply(\n",
        "        lambda x: \"item_\" + str(x)\n",
        "    )\n",
        "    feature_mapper = item_master.set_index(\"sld_menu_itm_id\")[\"feature_name\"].to_dict()\n",
        "    cat_mapper = item_master.set_index(\"feature_name\")[\"item_category\"].to_dict()\n",
        "    return feature_mapper, cat_mapper\n",
        "\n",
        "\n",
        "\n",
        "def items_to_model():\n",
        "    item_exclusions = LTO_ITEMS + DELISTED_ITEMS\n",
        "    df_item = pd.read_csv(ITEM_MASTER)\n",
        "    df_item_grp = pd.read_csv(ITEM_MAPPING)\n",
        "    df_item_group = df_item.merge(df_item_grp,on = ['sld_menu_itm_id'],how = 'left')\n",
        "    df_item_group['group_id'].fillna(df_item_group['sld_menu_itm_id'],inplace = True)\n",
        "    list_items = list(df_item_group['group_id'].unique().astype(int))\n",
        "    list_items = list (set(list_items)- set(item_exclusions))\n",
        "    return list_items\n",
        "    \n",
        "def wmape(y_true, y_pred):\n",
        "    \"\"\"Computes Weighted Mean Absolute Deviation.\n",
        "\n",
        "    Similar to mape but weighted by y_true.\n",
        "    This ignores y_true=0 and tends to be higher\n",
        "    if the errors are higher for higher y_true values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: actual values\n",
        "    y_pred: predicted values\n",
        "    \"\"\"\n",
        "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true)\n",
        "\n",
        "\n",
        "wmape_scorer = make_scorer(wmape, greater_is_better=False)\n",
        "\n",
        "def load_item_qty(y_data_path, item_master_path, item_grp_path, \n",
        "                  bundle_config_path, item_master_scaling_pth\n",
        "                  ,coop=None,start_date=\"2018-01-01\", \n",
        "                  end_date=\"2022-07-31\", item=None,store_id=None,store_grp=\"False\",\n",
        "                  store_lvl=\"False\",store_coop=\"False\", bundle_config_flag=True,\n",
        "                  varient_mapping=True,scaling=True):\n",
        "     \n",
        "    '''Load quantity data by filtering specific dates and segments.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    item_master : pd.DataFrame\n",
        "        DataFrame that contains items to be considered\n",
        "    item_grp : pd.DataFrame\n",
        "        DataFrame that contains parent grouping to the item_ids\n",
        "    start_date : str, default is \"2021-01-01\".\n",
        "        start date to filter data \n",
        "    end_date : str, default is \"2022-03-31\"\n",
        "        end_date to filter data.\n",
        "    path : str,\n",
        "        pass path of the loaction where data is kept.\n",
        "    coop : str,\n",
        "        pass coop name, list of coops for which data to be loaded.\n",
        "        \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Grouped data frame with specified filters.\n",
        "    '''\n",
        "    \n",
        "    # store_grp = model_constants.STORE_GROUPING\n",
        "    item_master = pd.read_csv(item_master_path)\n",
        "    item_grp = pd.read_csv(item_grp_path)\n",
        "\n",
        "    \n",
        "    if store_grp==\"True\" or store_lvl=='True' or store_coop==\"True\":\n",
        "        cols_ = ['mcd_gbal_lcat_id_nu','pos_busn_dt','sld_menu_itm_id', 'quantity', 'net_sales']\n",
        "    else:    \n",
        "        cols_ = ['pos_busn_dt', 'sld_menu_itm_id', 'quantity']\n",
        "    fils_ = []\n",
        "\n",
        "    if type(coop) is list:\n",
        "        fils_.append(('coop', 'in', set(coop))) \n",
        "    elif type(coop) is str:\n",
        "        if coop.lower() == \"national\" :\n",
        "            pass #No filter on coops and load all\n",
        "        else:\n",
        "            fils_.append(('coop', '=', coop))\n",
        "    else:\n",
        "        raise ValueError('Coop name is not correct')\n",
        "    \n",
        "    if start_date is not None:\n",
        "        fils_.append(('pos_busn_dt', '>=', start_date))\n",
        "    if end_date is not None:\n",
        "        fils_.append(('pos_busn_dt', '<=', end_date))\n",
        "    # add item filters\n",
        "\n",
        "    # if item is not None:\n",
        "    #     items = item_grp[item_grp.group_id.isin(item)].sld_menu_itm_id.to_list()\n",
        "    #     fils_.append(('sld_menu_itm_id','in',set(items)))\n",
        "\n",
        "    # if store_id is not None:\n",
        "    #     cols_.append('mcd_gbal_lcat_id_nu')\n",
        "    if varient_mapping == True:\n",
        "        if s3:\n",
        "            prefix_objs = bucket.objects.filter(Prefix=\"Pricing/cleaned/Quantity/coop_item_week\")\n",
        "            for obj in prefix_objs:\n",
        "                if 'SUCCESS' not in obj.key:\n",
        "                    path1 = f's3://{obj.bucket_name}/{obj.key}'\n",
        "                    #df = pd.read_parquet(path1)\n",
        "                    geo_name=obj.key.split(\"=\")[1].split(\"/\")[0].strip().upper()\n",
        "                    #print(geo_name)\n",
        "                    #print(geo_name)\n",
        "                    if geo_name=='unknown'.upper():\n",
        "                        continue\n",
        "                    if geo_name==coop.upper():\n",
        "                        print(geo_name,coop)\n",
        "                        break\n",
        "                    else:\n",
        "                        continue\n",
        "        # df = (\n",
        "        #     pq.ParquetDataset(path1, filters=fils_)\n",
        "        #     .read_pandas(columns=cols_)\n",
        "        #     .to_pandas()\n",
        "        #     .merge(item_grp[[\"sld_menu_itm_id\", \"group_id\"]],on=\"sld_menu_itm_id\",how=\"left\")\n",
        "        # )\n",
        "            df = pd.read_parquet(path1,columns=cols_,filters=fils_)\n",
        "            df = pd.merge(df,item_grp[[\"sld_menu_itm_id\", \"group_id\"]],on=\"sld_menu_itm_id\",how=\"left\")\n",
        "            df['coop'] = coop\n",
        "            #print(df.head(10))\n",
        "            print(\"yes\")\n",
        "            fil_ = df.group_id.isnull()\n",
        "            df.loc[fil_, \"group_id\"] = df.loc[fil_, \"sld_menu_itm_id\"]\n",
        "\n",
        "        else:\n",
        "           df = (\n",
        "            pq.ParquetDataset(y_data_path, filters=fils_)\n",
        "            .read_pandas(columns=cols_)\n",
        "            .to_pandas()\n",
        "            .merge(item_grp[[\"sld_menu_itm_id\", \"group_id\"]],on=\"sld_menu_itm_id\",how=\"left\")\n",
        "        )\n",
        "        df['coop'] = coop\n",
        "        fil_ = df.group_id.isnull()\n",
        "        df.loc[fil_, \"group_id\"] = df.loc[fil_, \"sld_menu_itm_id\"]\n",
        "   \n",
        "\n",
        "    else:\n",
        "        df = (\n",
        "            pq.ParquetDataset(y_data_path, filters=fils_)\n",
        "            .read_pandas(columns=cols_)\n",
        "            .to_pandas()\n",
        "        )\n",
        "        fil_ = df.group_id.isnull()\n",
        "        df.loc[fil_, \"group_id\"] = df.loc[fil_, \"sld_menu_itm_id\"]\n",
        "\n",
        "    if store_lvl=='True':\n",
        "        df=df[df['mcd_gbal_lcat_id_nu']==store_id].reset_index(drop = True)\n",
        "        \n",
        "    df['pos_busn_dt'] = df['pos_busn_dt'].astype(str)\n",
        "    df = df[(df['pos_busn_dt']>=start_date) & (df['pos_busn_dt']<=end_date)].reset_index(drop = True) \n",
        "\n",
        "   \n",
        "    if scaling and item_master_scaling_pth is not None :\n",
        "     \n",
        "        #item_master_scaling = pd.read_parquet(item_master_scaling_pth).query(\"Base_item_ID!='nan'\")\n",
        "        item_master_scaling = pd.read_parquet(item_master_scaling_pth).dropna(subset=['Base_item_ID'])#.query(\"Base_item_ID!='nan'\")\n",
        "        item_master_scaling['Base_item_ID']=item_master_scaling['Base_item_ID'].astype(np.int64).astype(str)\n",
        "        base_id_sf = item_master_scaling[['Scaling_Factor','sld_menu_itm_id']].set_index('sld_menu_itm_id').to_dict()['Scaling_Factor']\n",
        "        base_id_child = item_master_scaling[['sld_menu_itm_id','Base_item_ID']].set_index('sld_menu_itm_id').to_dict()['Base_item_ID']\n",
        "\n",
        "        df_new=df.copy()\n",
        "\n",
        "        for item_ in base_id_child.keys():\n",
        "\n",
        "            parent =  int(base_id_child[item_])\n",
        "\n",
        "            if df_new.query(\"sld_menu_itm_id==@item_\").shape[0]>0 and df_new.query(\"sld_menu_itm_id==@parent\").shape[0]>0:\n",
        "\n",
        "                \n",
        "                df_new['quantity'] = np.where(df_new['sld_menu_itm_id']==item_,base_id_sf[item_]*df_new['quantity'], df_new['quantity'])\n",
        "\n",
        "        \n",
        "\n",
        "                group_id_parent = df_new[df_new['sld_menu_itm_id']==parent]['group_id'].values[0]\n",
        "\n",
        "                df_new['group_id'] = np.where(df_new['sld_menu_itm_id']==item_,group_id_parent,df_new['group_id'])\n",
        "\n",
        "                df_new['sld_menu_itm_id'] = np.where(df_new['sld_menu_itm_id']==item_,parent, df_new['sld_menu_itm_id'].astype(np.int))\n",
        "\n",
        "               \n",
        "\n",
        "        \n",
        "        df=df_new\n",
        "\n",
        "\n",
        "\n",
        "    #df = pd.merge(df,store_binning,on='mcd_gbal_lcat_id_nu',how='left')\n",
        "    #df.drop(columns=['mcd_gbal_lcat_id_nu'],inplace=True)\n",
        "    #df.rename(columns={'median_minmax_bins':'mcd_gbal_lcat_id_nu'},inplace=True)\n",
        "    # Grouping\n",
        "    \n",
        "    df = pd_cols_to_datetime(df)\n",
        "    \n",
        "    if bundle_config_flag:\n",
        "        df['group_id'] = df['group_id'].astype(int)\n",
        "        #df['sld_menu_itm_id'] = df['sld_menu_itm_id'].astype(int)\n",
        "        df['pos_busn_dt']=pd.to_datetime(df['pos_busn_dt'])\n",
        "        df_bundle_mapping = pd.read_csv(bundle_config_path,usecols=['coop','pos_busn_dt','parent_id','child_id','qty_weight','sales_weight'])\n",
        "        df_bundle_mapping['coop']=df_bundle_mapping['coop'].apply(lambda x :str(x).lower().strip())\n",
        "        df_bundle_mapping['pos_busn_dt']=pd.to_datetime(df_bundle_mapping['pos_busn_dt'])\n",
        "        df_bundle_mapping['parent_id'] = df_bundle_mapping['parent_id'].astype(int)\n",
        "        \n",
        "        df=df.merge(df_bundle_mapping,left_on=['coop','pos_busn_dt','group_id'],right_on = ['coop','pos_busn_dt','parent_id'],how='left')\n",
        "        \n",
        "        #df['sld_menu_itm_id']=np.where(df['child_id'].isna(),df['sld_menu_itm_id'],df['child_id'])\n",
        "        df['group_id']=np.where(df['child_id'].isna(),df['group_id'],df['child_id'])        \n",
        "        \n",
        "        df['quantity']=np.where(df['child_id'].isna(),df['quantity'],df['quantity']*df['qty_weight'])\n",
        "        # df['offer_qty']=np.where(df['child_id'].isna(),df['offer_qty'],df['offer_qty']*df['qty_weight'])\n",
        "       \n",
        "        df.drop(columns=['child_id','qty_weight','sales_weight','parent_id'],inplace=True)\n",
        "    \n",
        "    df['group_id']=df['group_id'].astype(int)\n",
        "    df.coop = df.coop.astype(str)\n",
        "    \n",
        "    #filtering top items if not item list defined\n",
        "    if item is None:\n",
        "        li_items = item_master.sld_menu_itm_id.to_list()\n",
        "        df = df.query(\"group_id in @li_items\")\n",
        "    else:\n",
        "        df = df.query(\"group_id in @item\")\n",
        "\n",
        "    if type(coop) is str:\n",
        "        if store_grp=='True' or store_lvl=='True' or store_coop=='True':\n",
        "            df = (\n",
        "            df.groupby([\"mcd_gbal_lcat_id_nu\",\"pos_busn_dt\", \"group_id\"])\n",
        "            .agg(\n",
        "                quantity=('quantity','sum'),\n",
        "                #net_sales=('net_sales','sum'),\n",
        "            )\n",
        "            .reset_index()\n",
        "            .rename(columns={\"group_id\": \"dv_id\"})\n",
        "            .get_week_start_dt()\n",
        "        )\n",
        "            if coop.lower() == 'national':\n",
        "                df['coop'] = \"National\"\n",
        "            else:\n",
        "                df['coop'] = coop\n",
        "        else:\n",
        "            df = (\n",
        "                df.groupby([\"pos_busn_dt\", \"group_id\"])\n",
        "                .agg(\n",
        "                    quantity=('quantity','sum'),\n",
        "                    #net_sales=('net_sales','sum'),\n",
        "                )\n",
        "                .reset_index()\n",
        "                .rename(columns={\"group_id\": \"dv_id\"})\n",
        "                .get_week_start_dt()\n",
        "            )\n",
        "            if coop.lower() == 'national':\n",
        "                df['coop'] = \"National\"\n",
        "            else:\n",
        "                df['coop'] = coop\n",
        "    else :\n",
        "         df = (\n",
        "            df.groupby([\"coop\",\"pos_busn_dt\", \"group_id\"])\n",
        "            .agg(\n",
        "                quantity=('quantity','sum'),\n",
        "                #net_sales=('net_sales','sum'),\n",
        "            )\n",
        "            .reset_index()\n",
        "            .rename(columns={\"group_id\": \"dv_id\"})\n",
        "            .get_week_start_dt()\n",
        "        )\n",
        "    \n",
        "    return df\n",
        "\n",
        "def load_item_qty_data(df_item_qty_path,item_master_path,item_grp_path,coop,\n",
        "                       start_date,end_date,item,exclusion_dates = [],\n",
        "                       store_id=None,store_coop=None,bundle_config_path = None,\n",
        "                       scaling=True,item_master_scaling_path = None):\n",
        "\n",
        "    \"\"\"reads the item quantity path for a particular coop and returns the df with some cosmetic changes\"\"\"\n",
        "\n",
        "\n",
        "    store_grp=\"False\"\n",
        "    store_lvl=\"False\"\n",
        "    store_coop=\"False\"\n",
        "\n",
        "\n",
        "    df = load_item_qty(\n",
        "        y_data_path=df_item_qty_path,\n",
        "        item_master_path=item_master_path,\n",
        "        item_grp_path=item_grp_path,\n",
        "        coop=coop,\n",
        "        start_date = start_date,\n",
        "        end_date = end_date,\n",
        "        item=item,\n",
        "        store_id=store_id,\n",
        "        store_grp = store_grp,\n",
        "        store_lvl = store_lvl,\n",
        "        store_coop = store_coop,\n",
        "        bundle_config_path=bundle_config_path,\n",
        "        scaling= scaling,\n",
        "        item_master_scaling_pth = item_master_scaling_path,\n",
        "    )\n",
        "    df[\"pos_busn_dt\"] = pd.to_datetime(df[\"pos_busn_dt\"])\n",
        "    if len(exclusion_dates) > 0:\n",
        "        df = df[~df.pos_busn_dt.isin(exclusion_dates)]\n",
        "    rename_dict = {\"sld_menu_itm_id\": \"dv_id\", \"quantity\": \"y\"}\n",
        "    df.rename(columns=rename_dict, inplace=True)\n",
        "    if store_grp=='True' or store_lvl=='True' or store_coop == \"True\":\n",
        "        df = df[[\"mcd_gbal_lcat_id_nu\",\"pos_busn_dt\", \"dv_id\", \"y\"]]\n",
        "        df[\"estimated_measure\"] = \"item_qty\"\n",
        "    else:\n",
        "        df = df[[\"pos_busn_dt\", \"dv_id\", \"y\"]]\n",
        "        df[\"estimated_measure\"] = \"item_qty\"\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_gc_data(coop_up,start_date, end_date, gc_path_weekly,\n",
        "                exclusion_dates = [],s3=False,granularity='weekly',\n",
        "                 gc_path_daily=None):\n",
        "    \n",
        "    \"\"\"reads the gc path for a particular coop and returns the df with some cosmetic changes\"\"\"\n",
        "    \n",
        "    if s3:\n",
        "        import boto3\n",
        "        s3 = boto3.resource('s3')\n",
        "        bucket = s3.Bucket('us-east-1-cprod-us-spark-cluster')\n",
        "        prefix_objs = bucket.objects.filter(Prefix=\"Pricing/cleaned/GC/coop_day\")\n",
        "        for obj in prefix_objs:\n",
        "            if 'SUCCESS' not in obj.key:\n",
        "                path1 = f's3://{obj.bucket_name}/{obj.key}'\n",
        "                df = pd.read_parquet(path1)\n",
        "                geo_name=obj.key.split(\"=\")[1].split(\"/\")[0].strip().upper()\n",
        "                #print(geo_name)\n",
        "                if geo_name=='unknown'.upper():\n",
        "                    continue\n",
        "                if geo_name==coop_up:\n",
        "                    print(geo_name,coop_up)\n",
        "                    break\n",
        "                else:\n",
        "                    continue\n",
        "      \n",
        "        df['coop']=coop_up.lower()\n",
        "        df['pos_busn_dt']=pd.to_datetime(df['pos_busn_dt'])\n",
        "        df =df.query(\"pos_busn_dt>=@start_date and pos_busn_dt<=@end_date\").reset_index(drop=True)\n",
        "\n",
        "        \n",
        "\n",
        "        if len(exclusion_dates) > 0:\n",
        "            df = df[~df.pos_busn_dt.isin(exclusion_dates)]\n",
        "        rename_dict = { \"total_gc\": \"y\"}\n",
        "        df.rename(columns=rename_dict, inplace=True)\n",
        "        df = df.sort_values(\"pos_busn_dt\")\n",
        "        df = df[[\"pos_busn_dt\", \"coop\", \"y\"]]\n",
        "        df[\"estimated_measure\"] = \"gc\"\n",
        "        df=df.reset_index(drop=True)\n",
        "        #display(df.head())\n",
        "\n",
        "    else:\n",
        "\n",
        "        if granularity=='daily':\n",
        "\n",
        "            print(gc_path_daily,coop_up.lower()+'.parquet')\n",
        "            df = pd.read_parquet(gc_path_daily+coop_up.lower())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            print(gc_path_weekly+coop_up.lower()+'.parquet')\n",
        "            df = pd.read_parquet(gc_path_weekly+coop_up.lower())\n",
        "            \n",
        "        #print(df)\n",
        "        df['coop_name']=coop_up.lower()\n",
        "        df['pos_busn_dt']=pd.to_datetime(df['pos_busn_dt'])\n",
        "\n",
        "        df=df.query(\"pos_busn_dt>=@start_date and pos_busn_dt<=@end_date\")\n",
        "        if len(exclusion_dates) > 0:\n",
        "            df = df[~df.pos_busn_dt.isin(exclusion_dates)]\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        rename_dict = { \"total_gc\": \"y\"}\n",
        "        df.rename(columns=rename_dict, inplace=True)\n",
        "        df = df.sort_values(\"pos_busn_dt\")\n",
        "        df = df[[\"pos_busn_dt\", \"coop_name\", \"y\"]]\n",
        "        df[\"estimated_measure\"] = \"gc\"\n",
        "        df=df.reset_index(drop=True)\n",
        "                       \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHSpQvDk-vOx"
      },
      "source": [
        "#### Generating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpHWjHTQ_i4k"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(data_iter,\n",
        "                     output_path,\n",
        "                     qty_data_path,\n",
        "                     gc_data_path,\n",
        "                     item_grp_path,\n",
        "                     bundle_config_path,\n",
        "                     item_master_scaling,\n",
        "                     item_master_path,\n",
        "                     coops_list,\n",
        "                     new_exclusion_dates=None,\n",
        "                     save_train_data=True,\n",
        "                      ):\n",
        "  \n",
        "    if new_exclusion_dates is not None:\n",
        "      exclusion_dates.extend(new_exclusion_dates)\n",
        "\n",
        "    df_item_df = pd.DataFrame()\n",
        "    df_category_df = pd.DataFrame()\n",
        "\n",
        "    df_train = pd.DataFrame()\n",
        "\n",
        "\n",
        "    list_items = items_to_model()\n",
        "\n",
        "    topitems = return_top_items(ITEM_MASTER_ORG,X_OFFER_PATH,ITEM_MAPPING,ITEM_MASTER_SCALING)\n",
        "\n",
        "    topitems= [i for i in topitems if i in list_items]\n",
        "\n",
        "    print(len(list_items))\n",
        "    print(list_items)\n",
        "\n",
        "    print((topitems))\n",
        "    print(len(topitems))\n",
        "    \n",
        "    #category_items =  list(set(list_items)-set(topitems))\n",
        "\n",
        "\n",
        "    df1 = pd.DataFrame({'all_items':list_items})\n",
        "    df2 = pd.DataFrame({'top_items':list(topitems)})\n",
        "\n",
        "\n",
        "    items_log = pd.concat([df1,df2],axis=1)\n",
        "\n",
        " \n",
        "\n",
        "    item_master_df= pd.read_csv(item_master_path)\n",
        "    item_master_df.rename(columns={'sld_menu_itm_id':'dv_id'},inplace=True)\n",
        "\n",
        "\n",
        "    for coop in tqdm(coops_list):\n",
        "\n",
        "\n",
        "        print('---------------------------------')\n",
        "        print(f\"Total COOPs : {len(coops_list)}\")\n",
        "        print(f\"Current COOP Name : {coop}\")\n",
        "\n",
        "        ## load y data.\n",
        "        #top_items = list(item_config_analysis.query('Coop==@coop')['sld_menu_itm_id'])\n",
        "\n",
        "\n",
        "        st = time.time()    \n",
        "        y_qty = load_item_qty_data(df_item_qty_path = qty_data_path,\n",
        "                                   item_master_path = item_master_path,\n",
        "                                   item_grp_path = item_grp_path,\n",
        "                                   coop = coop,\n",
        "                                   start_date = start_date,\n",
        "                                   end_date = end_date,item = list_items,\n",
        "                                   exclusion_dates = exclusion_dates,\n",
        "                                   bundle_config_path=bundle_config_path,\n",
        "                                   item_master_scaling_path=item_master_scaling)\\\n",
        "                                  .rename(columns={'y':'actual_qty'})\\\n",
        "                                  .assign(estimated_measure='gc')\\\n",
        "                                  .assign(coop_name=coop)\n",
        "\n",
        "        y_qty['dv_id'] = y_qty['dv_id'].astype(np.int64)\n",
        "\n",
        "        y_gc= load_gc_data(coop.upper(), start_date, end_date,gc_data_path,\n",
        "                                exclusion_dates,s3=False,granularity=\"weekly\")\n",
        "        \n",
        "        base_price_cost = os.path.join(ONE_DATA_ROOT,'y_data','04_base_price_asp','raw',f'coop={coop}')\n",
        "\n",
        "        base_price_cost_denver = pd.read_parquet(base_price_cost).drop_duplicates() \n",
        "        base_price_cost_denver.rename(columns={'sld_menu_itm_id':'dv_id','week_st_dt':'pos_busn_dt'},inplace=True)\n",
        "        base_price_cost_denver['pos_busn_dt']=pd.to_datetime(base_price_cost_denver['pos_busn_dt'])\n",
        "        base_price_cost_denver['dv_id']=base_price_cost_denver['dv_id'].astype(int)\n",
        "\n",
        "        str_count_max=base_price_cost_denver.groupby(['pos_busn_dt'])['store_count'].max().reset_index()\n",
        "\n",
        "\n",
        "    \n",
        "        y_qty = y_qty.merge(base_price_cost_denver[['dv_id','store_count','pos_busn_dt']],on=['pos_busn_dt','dv_id'],how='left')\n",
        "        y_qty['actual_qty'] = y_qty['actual_qty']/y_qty['store_count']\n",
        "\n",
        "        y_qty.drop(columns=['store_count'],inplace=True)\n",
        "\n",
        "        y_gc=y_gc.merge(str_count_max,on=['pos_busn_dt'],how='left')\n",
        "\n",
        "        y_gc['y'] = y_gc['y']/y_gc['store_count']\n",
        "\n",
        "        y_gc.drop(columns=['store_count'],inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############ data prep ##########################\n",
        "        ################ category ########################\n",
        "\n",
        "        df_x_category = y_qty.merge(item_master_df[['dv_id','item_category']],on=['dv_id'],how='left').dropna()\n",
        "        df_x_category['estimated_measure'] ='gc'\n",
        "        \n",
        "        df_x_category = pd.pivot_table(df_x_category,values='actual_qty',columns='item_category',index=['pos_busn_dt','coop_name','estimated_measure'],aggfunc='sum').reset_index()\n",
        "        \n",
        "        ############# item ##########################\n",
        "        y_qty=y_qty.query('dv_id in @topitems')\n",
        "\n",
        "        y_qty['dv_id']='item_'+y_qty['dv_id'].astype(str)\n",
        "        df_x_item = pd.pivot_table(y_qty,values='actual_qty',columns='dv_id',index=['pos_busn_dt','coop_name','estimated_measure'],aggfunc='sum').reset_index()\n",
        "\n",
        "        ################# addding y data in x matrix ###################### \n",
        "        df1 = df_x_category.merge(y_gc,on=['coop_name','pos_busn_dt','estimated_measure'],how='inner')\n",
        "\n",
        "        df2 = df_x_item.merge(y_gc,on=['coop_name','pos_busn_dt','estimated_measure'],how='inner')\n",
        "\n",
        "        df_category_df= df_category_df.append(df1,ignore_index=True)\n",
        "\n",
        "        df_item_df= df_item_df.append(df2,ignore_index=True)\n",
        "\n",
        "        # df_x_y = df_x_category.merge(df_x_item,on=['coop_name','pos_busn_dt','estimated_measure'],how='outer')\\\n",
        "        #           .merge(y_gc,on=['coop_name','pos_busn_dt','estimated_measure'],how='outer')\n",
        "\n",
        "        # df_train=df_train.append(df_x_y,ignore_index=True)\n",
        "    \n",
        "    cols_to_drop =[col for col in df_category_df.columns if 'MULTI' in col or 'multi' in col  ]\n",
        "\n",
        "    df_category_df=df_category_df.drop(columns=cols_to_drop,axis=1)\n",
        "\n",
        "    output_dir = os.path.join(output_path,data_iter)\n",
        "\n",
        "    cat_dir = os.path.join(output_dir,'categories')\n",
        "    item_dir = os.path.join(output_dir,'items')\n",
        "\n",
        "\n",
        "    if os.path.exists(output_dir):\n",
        "        pass\n",
        "    else:\n",
        "        os.makedirs(output_dir)\n",
        "       \n",
        "\n",
        "    if os.path.exists(cat_dir):\n",
        "        pass\n",
        "    else:\n",
        "        os.makedirs(cat_dir)\n",
        "\n",
        "    if os.path.exists(item_dir):\n",
        "        pass\n",
        "    else:\n",
        "        os.makedirs(item_dir)\n",
        "\n",
        "    df_item_df.to_parquet(os.path.join(item_dir,'train.parquet'),engine='pyarrow')\n",
        "    df_category_df.to_parquet(os.path.join(cat_dir,'train.parquet'),engine='pyarrow')\n",
        "    items_log.to_csv(os.path.join(output_dir,'item_logger.csv'),index=False)\n",
        "\n",
        "    print(\"Dataset Created and saved @ \",output_dir)\n",
        "\n",
        "    return df_item_df,df_category_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umtFG4pC_qHh"
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #### CONFIG #####\n",
        "\n",
        "    data_iter='14'\n",
        "    coop_list = coops_top11\n",
        "    \n",
        "    data_prep_only=False\n",
        "    \n",
        "    cfg_demand_pipeline_iteration_num = 41\n",
        "    \n",
        "    cfg_gc_iteration_num = 18\n",
        "    \n",
        "    cfg_xgb_num_tasks = 1\n",
        "    cfg_xgb_num_threads_per_task = 8\n",
        "    \n",
        "\n",
        "    if sys.platform == \"win32\":\n",
        "        DRIVE_ROOT = r'G:\\Shared drives'\n",
        "    elif 'google.colab' in sys.modules:\n",
        "        DRIVE_ROOT = '/content/drive/Shareddrives'\n",
        "    else:\n",
        "        DRIVE_ROOT = \"/mnt/g/Shared drives\"\n",
        "\n",
        "    # ##### FOLDERS #####\n",
        "    USDE_ROOT = os.path.join(DRIVE_ROOT, \"MCD_Discount Engine\", \"US\", \"2022\")\n",
        "    \n",
        "    \n",
        "\n",
        "    MODELLING_ROOT = os.path.join(USDE_ROOT, \"4. Modeling\")\n",
        "    ONE_DATA_ROOT = os.path.join(USDE_ROOT, \"4. Modeling\",\"01_data\")\n",
        "\n",
        "    OUTPUT_DATA_ROOT = os.path.join(MODELLING_ROOT, \"01_data\",\"y_data\",\"02_gc_day_level\",\"Coop X Item X Categories\")\n",
        "    XGB_GC_ROOT = os.path.join(MODELLING_ROOT,\"03_gc_models\",\"xgb_models\")\n",
        "    \n",
        "    PIPELINE_ROOT = os.path.join(MODELLING_ROOT, \"99_pipeline\", str(cfg_demand_pipeline_iteration_num).zfill(2))\n",
        "    DEMAND_ROOT = os.path.join(PIPELINE_ROOT, \"01_demand\")\n",
        "    \n",
        "    X_OFFER_PATH = os.path.join(ONE_DATA_ROOT,'x_offers','05_offer_adjusted','weekly','depth')\n",
        "\n",
        "    ITEM_MASTER = os.path.join(ONE_DATA_ROOT,'item_master.csv')\n",
        "    ITEM_MASTER_ORG  =  os.path.join(DRIVE_ROOT, \"MCD_Discount Engine\", \"US\", \"2022\",\"2. Delivery Tracks\",\"EDA\",\"item\",\"item_master_internal_latest_v5.xlsx\")\n",
        "    ITEM_MAPPING = os.path.join(ONE_DATA_ROOT,'item_mapping.csv')\n",
        "    BUNDLE_CONFIG_DATA = os.path.join(ONE_DATA_ROOT,'bundle_config_ydata.csv')\n",
        "    ITEM_MASTER_SCALING= os.path.join(ONE_DATA_ROOT,\"item_master_scaling.parquet\")\n",
        "    QTY_DATA_PATH = os.path.join(USDE_ROOT, \"4. Modeling\",\"01_data\",\"y_data\",\n",
        "                                  \"01_item_day_level\",\"coop\",\"weekly\")\n",
        "    GC_DATA_PATH_DAILY = os.path.join(USDE_ROOT, \"4. Modeling\",\"01_data\",\"y_data\",\n",
        "                                      \"02_gc_day_level\",\"Coop X item X day\",\"coop=\")\n",
        "\n",
        "    GC_DATA_PATH_WEEKLY = os.path.join(USDE_ROOT, \"4. Modeling\",\"01_data\",\n",
        "                                       \"y_data\",\"02_gc_day_level\",\n",
        "                                       \"Coop X item X week\",\"coop=\")\n",
        "    \n",
        "    # #generate_dataset(data_iter,coop_list)\n",
        "\n",
        "    ITEM_INFO = os.path.join(ONE_DATA_ROOT,'sp_opt','inputs','item_info_current.parquet')\n",
        "    \n",
        "    \n",
        "    \n",
        "    MODELLING_INPUTS = os.path.join(MODELLING_ROOT, \"03_gc_models\",\"xgb_models\",\"parameters\")\n",
        "    \n",
        "    #MODEL_ITERATION_ROOT = os.path.join(MODELLING_ROOT, \"03_gc_models\",\"xgb_models\", str(cfg_gc_iteration_num))\n",
        "\n",
        "    MODEL_ITERATION_ROOT = os.path.join(MODELLING_ROOT, \"03_gc_models\",\"xgb_models\", str(cfg_gc_iteration_num))\n",
        "\n",
        "    # MODEL_ITERATION_ROOT = os.path.join(\n",
        "    #     USDE_ROOT, \"5. Workspaces\", \"Bharath\", \"GC\", \"iterations\", \"17\"\n",
        "    # )\n",
        "\n",
        "   \n",
        "    output_gc_predictions_filepath = os.path.join(\n",
        "        PIPELINE_ROOT, \"06_gc_xgb\", \"model_predictions.csv\"\n",
        "    )\n",
        "    output_gc_simulation_filepath = os.path.join(\n",
        "        PIPELINE_ROOT, \"06_gc_xgb\", \"gc_simulation_method_1.csv\"\n",
        "    )\n",
        "\n",
        "    GC_DATA_ROOT = os.path.join(OUTPUT_DATA_ROOT,data_iter)\n",
        "\n",
        "    # if os.path.exists(MODEL_ITERATION_ROOT):\n",
        "    #     pass\n",
        "    # else:\n",
        "    #   time.sleep(5)\n",
        "    #   shutil.copytree(MODELLING_INPUTS, MODEL_ITERATION_ROOT)\n",
        "    \n",
        "\n",
        "    # # ##### FILES #####\n",
        "\n",
        "    # #fp_item_master = os.path.join(MODELLING_INPUTS, \"item_master.csv\")\n",
        "\n",
        "    fp_gc_item_features = os.path.join(GC_DATA_ROOT,\"items\")\n",
        "    fp_gc_category_features = os.path.join(GC_DATA_ROOT,\"categories\")\n",
        "\n",
        "    fp_grid = os.path.join(MODEL_ITERATION_ROOT, \"grid.csv\")\n",
        "    fp_demand_predictions = os.path.join(DEMAND_ROOT, 'model_collated', 'model_predictions.parquet')\n",
        "    #output_tensor_path =  os.path.join(PIPELINE_ROOT,\"04_tensor\",\"naive_imputation\",\"all_tensor_raw.csv\")\n",
        "    output_tensor_path =  os.path.join(PIPELINE_ROOT,\"04_tensor\",\"tensor_processed\",\"all_tensor_processed_raw.csv\")\n",
        "    \n",
        "    cl_coop_col = \"coop_name\"\n",
        "    cl_response_col = \"y\"\n",
        "    cl_date_col = \"pos_busn_dt\"\n",
        "    cl_index_cols = [cl_date_col, \"estimated_measure\"]\n",
        "    dict_feature_mapper, dict_cat_mapper = _get_feature_cat_mapper(item_master_path=ITEM_MASTER)\n",
        "\n",
        "    #get store count as per info file\n",
        "\n",
        "    #str_cnt_mapper = pd.read_parquet(ITEM_INFO).groupby(['coop_name'])['stores_count'].max().to_dict()\n",
        "\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RztdNBa8733Z"
      },
      "source": [
        "### Creating the train dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZYjiMT0rjez",
        "outputId": "803b3be7-d4ef-4588-c54c-16dc4112a847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1500', '6323', '8941', '8946', '9842', '706', '9839', '5926', '382', '78', '366', '991', '2844', '922', '3655', '973', '6975', '592', '276', '7002', '1', '8944', '520', '9071', '35', '5280', '8476', '334', '9840', '148', '252', '4314', '116', '8932', '1557', '46', '981', '279', '383', '31', '483', '4', '9088', '2842', '3430', '3', '61', '900', '85', '250', '3832', '6183', '473', '903', '979', '9049', '8939', '5', '9841', '522', '521', '3065', '6050', '6110', '83', '337', '5093', '6053', '7001', '8950', '321', '9070', '6233', '277', '7', '3590', '914', '32', '60', '1852', '990', '948', '8936', '1245', '62', '918', '9089', '24', '4834', '7780', '901', '6173', '3499']\n",
            "['1500', '6323', '8941', '8946', '9842', '706', '9839', '5926', '382', '78', '366', '991', '2844', '922', '3655', '973', '6975', '592', '276', '7002', '1', '8944', '520', '9071', '35', '5280', '8476', '334', '9840', '148', '252', '4314', '116', '8932', '1557', '46', '981', '279', '383', '31', '483', '4', '9088', '2842', '3430', '3', '61', '900', '85', '250', '3832', '6183', '473', '903', '979', '9049', '8939', '5', '9841', '522', '521', '3065', '6050', '6110', '83', '337', '5093', '6053', '7001', '8950', '321', '9070', '6233', '277', '7', '3590', '914', '32', '60', '1852', '990', '948', '8936', '1245', '62', '918', '9089', '24', '4834', '7780', '901', '6173', '3499', '1500', '8941', '8946', '9842', '706', '9839', '5926', '382', '78', '991', '366', '2844', '4543', '922', '3655', '973', '6975', '592', '9033', '276', '7002', '1', '8944', '520', '9071', '35', '5280', '334', '148', '252', '949', '4314', '116', '8932', '1557', '46', '981', '279', '383', '31', '483', '4', '9088', '2842', '3430', '3', '61', '900', '85', '250', '3832', '6183', '473', '903', '979', '9049', '8939', '5', '9841', '971', '522', '521', '3065', '6050', '6110', '83', '337', '5093', '6053', '7001', '8950', '278', '321', '9070', '6233', '277', '7', '3590', '914', '32', '1852', '60', '990', '948', '8936', '1245', '62', '918', '9089', '24', '4834', '7780', '901', '6173', '3499']\n",
            "['1500', '6323', '8941', '8946', '9842', '706', '9839', '5926', '382', '78', '366', '991', '2844', '922', '3655', '973', '6975', '592', '276', '7002', '1', '8944', '520', '9071', '35', '5280', '8476', '334', '9840', '148', '252', '4314', '116', '8932', '1557', '46', '981', '279', '383', '31', '483', '4', '9088', '2842', '3430', '3', '61', '900', '85', '250', '3832', '6183', '473', '903', '979', '9049', '8939', '5', '9841', '522', '521', '3065', '6050', '6110', '83', '337', '5093', '6053', '7001', '8950', '321', '9070', '6233', '277', '7', '3590', '914', '32', '60', '1852', '990', '948', '8936', '1245', '62', '918', '9089', '24', '4834', '7780', '901', '6173', '3499', '1500', '8941', '8946', '9842', '706', '9839', '5926', '382', '78', '991', '366', '2844', '4543', '922', '3655', '973', '6975', '592', '9033', '276', '7002', '1', '8944', '520', '9071', '35', '5280', '334', '148', '252', '949', '4314', '116', '8932', '1557', '46', '981', '279', '383', '31', '483', '4', '9088', '2842', '3430', '3', '61', '900', '85', '250', '3832', '6183', '473', '903', '979', '9049', '8939', '5', '9841', '971', '522', '521', '3065', '6050', '6110', '83', '337', '5093', '6053', '7001', '8950', '278', '321', '9070', '6233', '277', '7', '3590', '914', '32', '1852', '60', '990', '948', '8936', '1245', '62', '918', '9089', '24', '4834', '7780', '901', '6173', '3499', '1500', '6323', '8941', '8946', '9842', '706', '9839', '5926', '382', '78', '366', '2844', '92', '3655', '6975', '592', '7002', '1', '9675', '8944', '3582', '6253', '5280', '35', '791', '334', '4283', '9840', '148', '251', '252', '99', '4314', '114', '8911', '116', '8932', '1557', '46', '383', '3426', '31', '483', '4', '369', '2842', '3430', '7009', '3', '61', '250', '85', '3832', '4297', '6183', '473', '8939', '5', '9841', '780', '522', '521', '1509', '3065', '87', '6050', '6110', '381', '83', '337', '5093', '370', '6053', '1043', '7001', '8950', '6651', '321', '6295', '6233', '368', '7', '3590', '32', '110', '60', '8936', '62', '6173', '7780', '4834', '3499']\n",
            "347\n",
            "[1, 2, 3, 4, 5, 6148, 7, 6, 1043, 24, 7194, 28, 6173, 30, 31, 32, 33, 35, 6183, 5159, 46, 4143, 3122, 60, 61, 62, 64, 3134, 1088, 3147, 3149, 78, 83, 85, 86, 87, 6233, 88, 92, 93, 4191, 4192, 4193, 98, 6243, 101, 100, 99, 2154, 107, 6253, 110, 4208, 3179, 4210, 114, 116, 5233, 5234, 6275, 132, 3217, 4247, 6295, 6136, 1179, 157, 158, 5280, 161, 163, 162, 6309, 167, 6323, 3251, 185, 4282, 187, 4283, 5306, 5307, 5309, 4288, 193, 1216, 1218, 4295, 4297, 1231, 1233, 4308, 213, 215, 4314, 4317, 6366, 6367, 224, 1245, 4322, 4323, 1249, 4329, 4333, 4334, 4335, 250, 251, 252, 259, 6408, 3336, 9491, 5401, 3357, 292, 1317, 1318, 1319, 5415, 2347, 7473, 1330, 1331, 1332, 5428, 1333, 321, 2371, 2373, 5446, 2375, 334, 336, 337, 2385, 5462, 9560, 1372, 5469, 1374, 354, 5474, 6500, 356, 358, 359, 3426, 361, 7522, 3429, 3430, 366, 5486, 368, 369, 370, 9585, 9591, 3448, 381, 382, 383, 2435, 2458, 2467, 3499, 7601, 4543, 1481, 9675, 473, 8666, 1500, 478, 4575, 483, 1508, 1509, 1510, 7653, 7659, 7661, 3572, 6651, 3581, 3582, 2565, 3590, 520, 521, 522, 2569, 6669, 1557, 2585, 14886, 14887, 550, 1603, 1604, 582, 14918, 3655, 592, 593, 594, 1617, 3673, 1625, 5723, 7780, 14956, 14955, 9839, 9840, 9841, 9842, 2679, 14969, 1659, 14972, 647, 1674, 5778, 8851, 4763, 5787, 671, 2721, 4770, 4771, 1697, 2725, 7843, 679, 6825, 682, 4777, 2731, 1710, 2743, 2746, 2753, 706, 1739, 9931, 8911, 719, 726, 727, 1754, 1756, 3809, 4834, 3810, 8932, 3811, 1765, 3815, 8936, 1768, 2794, 8938, 8939, 6893, 8941, 8944, 8945, 8946, 8947, 6900, 8948, 8950, 3832, 8954, 2827, 780, 791, 7960, 7961, 2842, 7963, 2844, 5918, 8992, 3875, 5923, 5926, 2857, 1840, 1841, 5944, 6975, 1858, 6986, 6994, 3924, 7001, 7002, 3933, 5982, 7009, 7010, 1892, 2922, 5996, 2939, 2940, 1916, 6013, 1929, 1930, 1931, 7056, 5008, 6050, 6053, 6060, 6071, 3000, 7103, 7104, 7113, 7114, 7116, 7117, 7118, 7119, 5077, 3030, 3031, 7129, 7130, 3036, 7132, 6110, 5087, 3041, 5093, 5101, 5104, 4082, 1016, 3065, 6138]\n",
            "[1, 3, 4, 5, 3590, 7, 520, 521, 522, 2565, 1043, 1557, 24, 6173, 31, 32, 35, 6183, 46, 60, 61, 62, 1603, 3655, 78, 592, 1617, 83, 85, 87, 6233, 5723, 92, 99, 7780, 100, 2154, 6253, 110, 9839, 9840, 9841, 9842, 114, 116, 5778, 6295, 671, 5280, 2721, 162, 6323, 185, 4283, 1216, 706, 1218, 4295, 4297, 8911, 719, 4314, 1245, 1249, 4834, 8932, 8936, 8939, 8941, 8944, 8946, 8950, 3832, 250, 251, 252, 780, 791, 2842, 7961, 2844, 7963, 5926, 5415, 1330, 1332, 6975, 321, 2371, 334, 337, 7002, 7001, 7009, 3426, 6500, 3429, 3430, 361, 366, 368, 369, 370, 381, 382, 383, 6050, 6053, 3499, 4543, 9675, 7117, 3030, 3031, 473, 7129, 1500, 6110, 3041, 483, 5093, 1509, 7653, 3065, 6651, 3582]\n",
            "126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/11 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : kansas city nebraska sioux city omaha\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=kansas city nebraska sioux city omaha.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 1/11 [00:20<03:26, 20.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : sacramento fresno reno bakersfield\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=sacramento fresno reno bakersfield.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 2/11 [00:29<02:04, 13.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : washington dc baltimore eastern shore\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=washington dc baltimore eastern shore.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 3/11 [00:39<01:35, 11.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : dallas tyler\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=dallas tyler.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▋      | 4/11 [00:50<01:21, 11.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : denver col springs s colorado\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=denver col springs s colorado.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 5/11 [00:58<01:01, 10.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : triad wilmington sw va wi s gnw\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=triad wilmington sw va wi s gnw.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 6/11 [01:08<00:50, 10.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : albuquerque el paso\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=albuquerque el paso.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▎   | 7/11 [01:17<00:39,  9.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : atlanta nw georgia\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=atlanta nw georgia.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 8/11 [01:25<00:27,  9.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : pittsburgh johnstown erie\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=pittsburgh johnstown erie.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 9/11 [01:35<00:19,  9.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : philadelphia\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=philadelphia.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 10/11 [01:44<00:09,  9.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "Total COOPs : 11\n",
            "Current COOP Name : st louis\n",
            "/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X item X week/coop=st louis.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:52<00:00, 10.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Created and saved @  /content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X Item X Categories/14\n"
          ]
        }
      ],
      "source": [
        "if generate_training_dataset:\n",
        "  \n",
        "  new_exclusions_dates= [str(i.date()) for i in pd.date_range('2022-07-01','2022-07-31')]\n",
        "\n",
        "  generate_dataset( data_iter=data_iter,\n",
        "                      output_path=OUTPUT_DATA_ROOT,\n",
        "                      qty_data_path=QTY_DATA_PATH,\n",
        "                   \n",
        "                      gc_data_path =GC_DATA_PATH_WEEKLY,\n",
        "                      item_grp_path=ITEM_MAPPING,\n",
        "                      bundle_config_path=BUNDLE_CONFIG_DATA,\n",
        "                      item_master_scaling=ITEM_MASTER_SCALING,\n",
        "                      item_master_path=ITEM_MASTER,\n",
        "                      coops_list=coops_top11,\n",
        "                      new_exclusion_dates=new_exclusions_dates,\n",
        "                      save_train_data=generate_training_dataset)\n",
        "else:\n",
        "    print(\"Training Dataset not created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "id": "pGWnqpg6-o-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Store Normalized Data"
      ],
      "metadata": {
        "id": "XnqjtyEaKFu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_iter=8\n",
        "\n",
        "# df_categories = pd.read_parquet(os.path.join(OUTPUT_DATA_ROOT,str(data_iter),'categories','train.parquet'))\n",
        "# df_items = pd.read_parquet(os.path.join(OUTPUT_DATA_ROOT,str(data_iter),'items','train.parquet'))\n",
        "\n",
        "# df_categories['str_count'] = df_categories['coop_name'].map(str_cnt_mapper)\n",
        "# df_items['str_count'] = df_items['coop_name'].map(str_cnt_mapper)\n"
      ],
      "metadata": {
        "id": "8eXgPNKuTB9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# categories=set(dict_cat_mapper.values())\n",
        "# for col in df_categories.columns:\n",
        "#   if col in categories or col=='y':\n",
        "#     df_categories[col] = df_categories[col]/df_categories['str_count']\n",
        "# for col in df_items.columns:\n",
        "#   if 'item' in col or col=='y':\n",
        "#     df_items[col] = df_items[col]/df_items['str_count']\n",
        "# df_categories.drop(columns=['str_count'],inplace=True)\n",
        "# df_items.drop(columns=['str_count'],inplace=True)"
      ],
      "metadata": {
        "id": "rblRxatDThSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# df_categories.to_parquet(os.path.join(OUTPUT_DATA_ROOT,str(data_iter+1),'categories','train.parquet'))\n",
        "# df_items.to_parquet(os.path.join(OUTPUT_DATA_ROOT,str(data_iter+1),'items','train.parquet'))"
      ],
      "metadata": {
        "id": "TTq9pYTzKFHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling for category"
      ],
      "metadata": {
        "id": "kk5iUli7x6Ni"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSqyAyTs8A3Z"
      },
      "source": [
        "#### Training XgBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkM-avgEnpbB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class YMTimeSeriesSplitter(object):\n",
        "    @staticmethod\n",
        "    def _encode(x):\n",
        "        return str(x.year) + \"-\" + str(x.month).zfill(2)\n",
        "\n",
        "    def _divide(self, df_train):\n",
        "        train_times = [item[self._date_idx_dim] for item in\n",
        "                       df_train.index.tolist()] if self._date_idx_dim is not None else df_train.index.tolist()\n",
        "        train_times = pd.Series(sorted(list(set([YMTimeSeriesSplitter._encode(item) for item in train_times]))))\n",
        "        self._fold_map = pd.merge(\n",
        "            pd.DataFrame({'part': train_times}),\n",
        "            pd.DataFrame([{\n",
        "                'part': item, 'fold': fold_idx + 1\n",
        "            } for fold_idx, (_, outfold_index) in enumerate(\n",
        "                TimeSeriesSplit(\n",
        "                    n_splits=self._nsplits,\n",
        "                ).split(train_times)\n",
        "            ) for item in train_times[outfold_index].tolist()\n",
        "            ]),\n",
        "            on='part',\n",
        "            how='left'\n",
        "        ).fillna(0).set_index('part').to_dict()['fold']\n",
        "\n",
        "    def get_n_splits(self, X=None, y=None, groups=None):\n",
        "        return self._nsplits\n",
        "\n",
        "    def split(self, X=None, y=None, groups=None):\n",
        "        dates = X.index.to_frame().reset_index(drop=True)\n",
        "        time_col = dates.columns.tolist()[self._date_idx_dim] if self._date_idx_dim is not None else 0\n",
        "        dates['fold'] = dates[time_col].apply(lambda x: self._fold_map[YMTimeSeriesSplitter._encode(x)]).astype(int)\n",
        "        for fold_idx in range(1, self._nsplits + 1):\n",
        "            train_index = dates[dates['fold'] < fold_idx].index.tolist()\n",
        "            test_index = dates[dates['fold'] == fold_idx].index.tolist()\n",
        "            yield (train_index, test_index)\n",
        "\n",
        "    def __init__(self, train_df, date_idx_dim=0, n_splits=5) -> None:\n",
        "        \"\"\"\n",
        "        Implements a rolling time series (year-month) based cross validation\n",
        "        \"\"\"\n",
        "        self._nsplits = n_splits\n",
        "        self._date_idx_dim = date_idx_dim\n",
        "        self._divide(train_df)\n",
        "\n",
        "\n",
        "class ModelTuner(object):\n",
        "    def _make_grid(self, grid_filepath, kwargs):\n",
        "        \"\"\"\n",
        "        TODO - Support for experiment_number\n",
        "\n",
        "        Builds the grid from the grid file and the kwargs\n",
        "\n",
        "        Grid File CSV Format:\n",
        "            param_name,spec_type,spec\n",
        "        \"\"\"\n",
        "        for rec in pd.read_csv(grid_filepath).to_dict(orient=\"records\"):\n",
        "            if rec['param_name'] in kwargs:\n",
        "                continue\n",
        "            if rec[\"spec_type\"] == \"range\":\n",
        "                self._param_grid[rec[\"param_name\"]] = np.arange(*json.loads(rec[\"spec\"]))\n",
        "            elif rec[\"spec_type\"] == \"list\":\n",
        "                self._param_grid[rec[\"param_name\"]] = json.loads(rec[\"spec\"])\n",
        "            else:\n",
        "                raise ValueError(\"Unknown spec_type: {}\".format(rec[\"spec_type\"]))\n",
        "        for key, value in kwargs.items():\n",
        "            self._estimator_params[key] = value\n",
        "\n",
        "    def _grid_search(self, train_df, actuals_col, n_splits, date_idx_dim, n_tasks, verbose):\n",
        "        estimator = self._estimator_fn(**self._estimator_params)\n",
        "        self._gsearch = GridSearchCV(\n",
        "            estimator=estimator,\n",
        "            param_grid=self._param_grid,\n",
        "            scoring=self._scorer_fn,\n",
        "            n_jobs=n_tasks,\n",
        "            cv=YMTimeSeriesSplitter(train_df, n_splits=n_splits, date_idx_dim=date_idx_dim),\n",
        "            verbose=verbose\n",
        "        )\n",
        "        self._gsearch.fit(train_df.drop(actuals_col, axis=1), train_df[actuals_col])\n",
        "\n",
        "    def _build_final_model(self, df_train, actuals_col):\n",
        "        final_estimator_params = {k: v for k, v in self._estimator_params.items()}\n",
        "        final_estimator_params.update(self._gsearch.best_params_)\n",
        "        self._best_model = self._estimator_fn(**final_estimator_params)\n",
        "        self._best_model.fit(df_train.drop(actuals_col, axis=1), df_train[actuals_col])\n",
        "\n",
        "    def get_best_model(self):\n",
        "        return self._best_model\n",
        "\n",
        "    def get_tuning_results(self):\n",
        "        df = pd.DataFrame(self._gsearch.cv_results_).copy()\n",
        "        for k, v in self._estimator_params.items():\n",
        "            df[k] = json.dumps(v)\n",
        "        return df\n",
        "\n",
        "    def run(self, data_df, actuals_col, n_splits=3, date_idx_dim=0, n_tasks=1, verbose=3):\n",
        "        self._grid_search(data_df, actuals_col, n_splits, date_idx_dim, n_tasks, verbose)\n",
        "        self._build_final_model(data_df, actuals_col)\n",
        "        return self\n",
        "\n",
        "    def __init__(self, estimator_fn, grid_filepath, scorer_fn, **kwargs):\n",
        "        self._estimator_fn = estimator_fn\n",
        "        self._scorer_fn = scorer_fn\n",
        "        self._grid_filepath = grid_filepath\n",
        "        self._estimator_params = {}\n",
        "        self._param_grid = {}\n",
        "        self._gsearch = {}\n",
        "        self._best_model = None\n",
        "        self._make_grid(grid_filepath, kwargs)\n",
        "\n",
        "\n",
        "class BaseModel(object):\n",
        "    def y_transform_fwd(self, y):\n",
        "        return y\n",
        "\n",
        "    def y_transform_rev(self, y):\n",
        "        return y\n",
        "\n",
        "    def save(self, filepath, save_tuner=True):\n",
        "        data = {\n",
        "            'estimator': self._estimator,\n",
        "            'control_features': self._control_features,\n",
        "            'self_features': self._self_features,\n",
        "            'subst_features': self._subst_features,\n",
        "            'compl_features': self._compl_features,\n",
        "            'no_offer_level': self._no_offer_level\n",
        "        }\n",
        "        if save_tuner:\n",
        "            data['tuner'] = self._tuner\n",
        "        with open(filepath + \".pkl\" if not filepath.endswith(\".pkl\") else filepath, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        with open(filepath + \".pkl\" if not filepath.endswith(\".pkl\") else filepath, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self._estimator = data['estimator']\n",
        "        self._control_features = data['control_features']\n",
        "        self._self_features = data['self_features']\n",
        "        self._subst_features = data['subst_features']\n",
        "        self._compl_features = data['compl_features']\n",
        "        self._no_offer_level = data['no_offer_level']\n",
        "        if 'tuner' in data:\n",
        "            self._tuner = data['tuner']\n",
        "        return self\n",
        "\n",
        "    def get_features(self, offer_only=False):\n",
        "        if offer_only:\n",
        "            return self._self_features + self._subst_features + self._compl_features\n",
        "        return self._control_features + self._self_features + self._subst_features + self._compl_features\n",
        "\n",
        "    def get_model(self):\n",
        "        return self._estimator\n",
        "\n",
        "    def get_tuner(self):\n",
        "        return self._tuner\n",
        "\n",
        "    def get_no_offer_level(self):\n",
        "        return self._no_offer_level\n",
        "\n",
        "    def fit(self, data_df, actuals_col, grid_filepath, date_idx_dim=0, n_tasks=1, n_threads_per_task=8, n_splits=3,\n",
        "            verbose=1):\n",
        "        params = {k: v for k, v in self._estimator_params.items()}\n",
        "        params['n_jobs'] = n_threads_per_task\n",
        "        df = data_df[self.get_features() + [actuals_col]].fillna(self._no_offer_level)\n",
        "        df[actuals_col] = self.y_transform_fwd(df[actuals_col])\n",
        "        self._tuner = ModelTuner(\n",
        "            estimator_fn=self._estimator_fn,\n",
        "            grid_filepath=grid_filepath,\n",
        "            scorer_fn=self._scorer_fn,\n",
        "            **params\n",
        "        ).run(\n",
        "            data_df=df,\n",
        "            actuals_col=actuals_col,\n",
        "            date_idx_dim=date_idx_dim,\n",
        "            n_tasks=n_tasks,\n",
        "            n_splits=n_splits,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        self._estimator = self._tuner.get_best_model()\n",
        "        return self\n",
        "\n",
        "    def predict(self, data_df, actuals_col=None, keep_other_cols=True, data_df_base=None):\n",
        "        X = data_df[self.get_features()].fillna(self._no_offer_level)\n",
        "        if data_df_base is not None:\n",
        "            X_base = data_df_base[self.get_features()].fillna(self._no_offer_level)\n",
        "        else:\n",
        "            X_base = X.copy()\n",
        "            X_base[self.get_features(offer_only=True)] = self._no_offer_level\n",
        "        if keep_other_cols:\n",
        "            result = data_df[[col for col in data_df.columns if col not in self.get_features()]].reset_index()\n",
        "        else:\n",
        "            result = data_df.index.to_frame()\n",
        "        if actuals_col is not None and actuals_col in X:\n",
        "            result['y'] = data_df[actuals_col]\n",
        "        result['yhatbase'] = self._estimator.predict(X_base)\n",
        "        result['yhat'] = self._estimator.predict(X)\n",
        "        result['yhatbase'] = self.y_transform_rev(result['yhatbase'])\n",
        "        result['yhat'] = self.y_transform_rev(result['yhat'])\n",
        "        return result\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath):\n",
        "        return cls(control_features=[], self_features=[], subst_features=[], compl_features=[]).load(filepath)\n",
        "\n",
        "    def __init__(self, control_features, self_features, subst_features, compl_features, no_offer_level=0):\n",
        "        self._control_features = [col for col in control_features]\n",
        "        self._self_features = [col for col in self_features]\n",
        "        self._subst_features = [col for col in subst_features]\n",
        "        self._compl_features = [col for col in compl_features]\n",
        "        self._no_offer_level = no_offer_level\n",
        "        self._tuner = None\n",
        "        self._estimator = None\n",
        "        self._estimator_fn = None\n",
        "        self._estimator_params = {}\n",
        "        self._scorer_fn = None\n",
        "\n",
        "\n",
        "class GBDTRegressionModel(BaseModel):\n",
        "    def _set_constraints(self):\n",
        "        monotone_constraints = {}\n",
        "        for feature in self._self_features:\n",
        "            monotone_constraints[feature] = 1\n",
        "        for feature in self._subst_features:\n",
        "            monotone_constraints[feature] = -1\n",
        "        for feature in self._compl_features:\n",
        "            monotone_constraints[feature] = 1\n",
        "        self._estimator_params['monotone_constraints'] = monotone_constraints\n",
        "\n",
        "    def get_variable_importance(self):\n",
        "        vi = pd.DataFrame({\n",
        "            'feature': self.get_features(),\n",
        "            'importance': self._estimator.feature_importances_\n",
        "        })\n",
        "        vi.loc[vi['feature'].isin(self._control_features), 'feature_type'] = 'control'\n",
        "        vi.loc[vi['feature'].isin(self._self_features), 'feature_type'] = 'self'\n",
        "        vi.loc[vi['feature'].isin(self._subst_features), 'feature_type'] = 'substitute'\n",
        "        vi.loc[vi['feature'].isin(self._compl_features), 'feature_type'] = 'complement'\n",
        "        vi['contribution'] = 100 * vi['importance'] / vi['importance'].sum()\n",
        "        vi['relative_importance'] = 100 * vi['importance'] / vi['importance'].max()\n",
        "        return vi[[\n",
        "            'feature', 'feature_type', 'contribution', 'relative_importance'\n",
        "        ]].sort_values('contribution', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    def get_split_points(self):\n",
        "        tree_df = self._estimator.get_booster().trees_to_dataframe()\n",
        "        tree_df = tree_df[tree_df['Feature'].isin(self.get_features(offer_only=True))][\n",
        "            ['Feature', 'Split']].reset_index(drop=True)\n",
        "        return tree_df.groupby('Feature')['Split'].apply(lambda x: sorted(list(set([0] + list(x))))).to_dict()\n",
        "\n",
        "    def enable_gpu(self):\n",
        "        self._estimator_params['tree_method'] = 'gpu_hist'\n",
        "        self._estimator_params['predictor'] = 'gpu_predictor'\n",
        "\n",
        "    def __init__(self, control_features, self_features, subst_features, compl_features, no_offer_level=0, **kwargs):\n",
        "        super(GBDTRegressionModel, self).__init__(control_features, self_features, subst_features, compl_features,\n",
        "                                                  no_offer_level)\n",
        "        self._estimator_params = {\n",
        "            'tree_method': 'exact',\n",
        "            'random_state': 42\n",
        "        }\n",
        "        self._set_constraints()\n",
        "        self._estimator_params.update(kwargs)\n",
        "        self._estimator_fn = XGBRegressor\n",
        "        self._scorer_fn = wmape_scorer\n",
        "\n",
        "\n",
        "\n",
        "def _get_modelling_data(\n",
        "        item_features_path,\n",
        "        category_features_path,\n",
        "        index_cols,\n",
        "        coop_col,\n",
        "        response_col,\n",
        "        test_simulation=False\n",
        "):\n",
        "    train_df = pd.merge(\n",
        "        pd.read_parquet(os.path.join(item_features_path, \"train.parquet\")).rename(columns={'coop': coop_col}),\n",
        "        pd.read_parquet(os.path.join(category_features_path, \"train.parquet\")).rename(columns={'coop': coop_col}),\n",
        "        on=index_cols + [coop_col, response_col]\n",
        "    ).fillna(0)\n",
        "\n",
        "    if test_simulation:\n",
        "\n",
        "      test_df = pd.merge(\n",
        "          pd.read_parquet(os.path.join(item_features_path, \"test.parquet\")).rename(columns={'coop': coop_col}),\n",
        "          pd.read_parquet(os.path.join(category_features_path, \"test.parquet\")).rename(columns={'coop': coop_col}),\n",
        "          on=index_cols + [coop_col]\n",
        "      ).fillna(0)\n",
        "\n",
        "    else:\n",
        "      test_df = pd.DataFrame()\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "def fit_model(gc_item_features_path, gc_category_features_path, index_cols, coop_col, response_col, grid_filepath,\n",
        "              xgb_num_tasks, xgb_num_threads_per_task, iteration_root):\n",
        "    train_df, _ = _get_modelling_data(\n",
        "        item_features_path=gc_item_features_path,\n",
        "        category_features_path=gc_category_features_path,\n",
        "        index_cols=index_cols,\n",
        "        coop_col=coop_col,\n",
        "        response_col=response_col\n",
        "    )\n",
        "\n",
        "\n",
        "    self_features = [col for col in train_df.columns if\n",
        "                     col in dict_feature_mapper.values() or col in dict_cat_mapper.values()]\n",
        "    model = GBDTRegressionModel(\n",
        "        self_features=self_features,\n",
        "        control_features=[],\n",
        "        subst_features=[],\n",
        "        compl_features=[],\n",
        "        no_offer_level=0\n",
        "    )\n",
        "    if 'google.colab' in sys.modules:\n",
        "        model.enable_gpu()\n",
        "    model = model.fit(\n",
        "        data_df=train_df.set_index(index_cols),\n",
        "        actuals_col=response_col,\n",
        "        grid_filepath=grid_filepath,\n",
        "        n_tasks=xgb_num_tasks,\n",
        "        n_threads_per_task=xgb_num_threads_per_task\n",
        "    )\n",
        "    model.save(os.path.join(iteration_root, \"model.pkl\"))\n",
        "    model.get_tuner().get_tuning_results().to_csv(\n",
        "        os.path.join(iteration_root, \"tuning_results.csv\")\n",
        "    )\n",
        "    model.get_variable_importance().to_csv(os.path.join(iteration_root, \"vi.csv\"))\n",
        "    model.get_model().save_model(os.path.join(iteration_root, \"model.ubj\"))\n",
        "    train_predictions = model.predict(train_df.set_index(index_cols))\n",
        "\n",
        "\n",
        "    train_df['yhat'] = train_predictions['yhat']\n",
        "\n",
        "\n",
        "    train_predictions['split'] = 'train'\n",
        "    train_predictions['y'] = train_df['y']\n",
        "    \n",
        "    #test_predictions = model.predict(test_df.set_index(index_cols))\n",
        "\n",
        "    #test_df['y']=0\n",
        "    #test_df['yhat'] = test_predictions['yhat']\n",
        "\n",
        "    #test_predictions['split'] = 'test'\n",
        "\n",
        "    \n",
        "    #test_predictions['y'] = 0\n",
        "\n",
        "\n",
        "    #predictions = pd.concat([train_predictions, test_predictions])\n",
        "\n",
        "    # print(train_predictions)\n",
        "    # print(test_predictions)\n",
        "    # print(predictions)\n",
        "\n",
        "    # metrics = pd.merge(\n",
        "    #     train_df.groupby(coop_col).apply(lambda x: wmape(x['y'], x['yhat'])).reset_index().rename(\n",
        "    #         columns={0: 'wmape_train'}),\n",
        "    #     test_df.groupby(coop_col).apply(lambda x: wmape(x['y'], x['yhat'])).reset_index().rename(\n",
        "    #         columns={0: 'wmape_test'}),\n",
        "    #     on=coop_col\n",
        "    # )\n",
        "\n",
        "    metrics = train_df.groupby(coop_col).apply(lambda x: wmape(x['y'], x['yhat'])).reset_index().rename(\n",
        "           columns={0: 'wmape_train'})\n",
        "    \n",
        "    train_predictions.reset_index().to_csv(os.path.join(iteration_root, \"predictions.csv\"))\n",
        "    metrics.to_csv(os.path.join(iteration_root, \"metrics.csv\"))\n",
        "    return model\n",
        "    \n",
        "def multiindex_pivot(df, index=None, columns=None, values=None):\n",
        "    # https://github.com/pandas-dev/pandas/issues/23955\n",
        "    if index is None:\n",
        "        names = list(df.index.names)\n",
        "        df = df.reset_index()\n",
        "    else:\n",
        "        names = index\n",
        "    list_index = df[names].values\n",
        "    tuples_index = [tuple(i) for i in list_index]  # hashable\n",
        "    df = df.assign(tuples_index=tuples_index)\n",
        "    df = df.pivot(index=\"tuples_index\", columns=columns, values=values)\n",
        "    tuples_index = df.index  # reduced\n",
        "    index = pd.MultiIndex.from_tuples(tuples_index, names=names)\n",
        "    df.index = index\n",
        "    return df\n",
        "\n",
        "\n",
        "def _predict_from_demand(\n",
        "    model,\n",
        "    demand_predictions_path,\n",
        "    date_col,\n",
        "    coop_col,\n",
        "    cat_mapper,\n",
        "    index_cols,\n",
        "    baseline,\n",
        "    corrected_base,\n",
        "    store_level\n",
        "):\n",
        "    if baseline:\n",
        "        if corrected_base:\n",
        "            pred_col = \"yhatbase_corrected\"\n",
        "        else:\n",
        "            pred_col = \"yhatbase\"\n",
        "    else:\n",
        "        pred_col = \"yhat\"\n",
        "\n",
        "    gc_input = pd.read_parquet(demand_predictions_path)\n",
        "    \n",
        "    \n",
        "    \n",
        "    gc_input[\"feature_name\"] = gc_input[\"dv_id\"].apply(lambda x: \"item_\" + str(x))\n",
        "    gc_input = gc_input[\n",
        "        [date_col, coop_col, \"feature_name\", pred_col, \"split\"]\n",
        "\n",
        "\n",
        "    ].reset_index(drop=True)\n",
        "    gc_input[\"estimated_measure\"] = \"gc\"\n",
        "    gc_input[\"category\"] = gc_input[\"feature_name\"].map(cat_mapper)\n",
        "\n",
        "    # base_gc_items=base_gc.pipe(multiindex_pivot, index=index_cols + [coop_col], columns='feature_name', values='yhatbase').fillna(0).reset_index()\n",
        "    # gc_input_items = gc_input.pivot(\n",
        "    #     index=index_cols + [coop_col,'split'],\n",
        "    #     columns=\"feature_name\",\n",
        "    #     values=pred_col\n",
        "    # ).fillna(0).reset_index()\n",
        "\n",
        "    gc_input_items = (\n",
        "        gc_input.pipe(\n",
        "            multiindex_pivot,\n",
        "            index=index_cols + [coop_col, \"split\"],\n",
        "            columns=\"feature_name\",\n",
        "            values=pred_col,\n",
        "        )\n",
        "        .fillna(0)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "\n",
        "    # gc_input_cat = gc_input[index_cols + [coop_col, 'category', pred_col,'split']].groupby(\n",
        "    #     index_cols + [coop_col, 'category','split']\n",
        "    # )[pred_col].sum().reset_index().pivot(\n",
        "    #     index=index_cols + [coop_col,'split'],\n",
        "    #     columns='category',\n",
        "    #     values=pred_col\n",
        "    # ).fillna(0).reset_index()\n",
        "    \n",
        "    ## rescaling for categories \n",
        "    #gc_input = pd.read_parquet(demand_predictions_path)\n",
        "    #gc_input= scaling_for_category(gc_input,ITEM_MASTER,pred_col)\n",
        "    #gc_input[\"feature_name\"] = gc_input[\"dv_id\"].apply(lambda x: \"item_\" + str(x))\n",
        "    # gc_input = gc_input[\n",
        "    #     [date_col, coop_col, \"feature_name\", pred_col, \"split\"] ]\n",
        "\n",
        "    # gc_input[\"estimated_measure\"] = \"gc\"\n",
        "    gc_input[\"category\"] = gc_input[\"feature_name\"].map(cat_mapper)\n",
        "\n",
        "    gc_input_cat = (\n",
        "        gc_input[index_cols + [coop_col, \"category\", pred_col, \"split\"]]\n",
        "        .groupby(index_cols + [coop_col, \"category\", \"split\"])[pred_col]\n",
        "        .sum()\n",
        "        .reset_index()\n",
        "        .pipe(\n",
        "            multiindex_pivot,\n",
        "            index=index_cols + [coop_col, \"split\"],\n",
        "            columns=\"category\",\n",
        "            values=pred_col,\n",
        "        )\n",
        "        .fillna(0)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    if store_level:\n",
        "      gc_input_items['str_count']= gc_input_items[coop_col].map(str_cnt_mapper)\n",
        "      gc_input_cat['str_count']= gc_input_cat[coop_col].map(str_cnt_mapper)\n",
        "      \n",
        "      for col in gc_input_items.columns:\n",
        "        if 'item' in col:\n",
        "          gc_input_items[col] = gc_input_items[col]/gc_input_items['str_count']\n",
        "\n",
        "      \n",
        "      for col in gc_input_cat.columns:\n",
        "        if col in set(cat_mapper.values()):\n",
        "          gc_input_cat[col] =  gc_input_cat[col]/gc_input_cat['str_count']\n",
        "\n",
        "      gc_input_items.drop(columns=['str_count'],inplace=True)\n",
        "      gc_input_cat.drop(columns=['str_count'],inplace=True)\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "    gc_input = pd.merge(gc_input_items, gc_input_cat, on=index_cols + [coop_col, \"split\"])\n",
        "    gc_input = pd.concat(\n",
        "        [\n",
        "            gc_input,\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    col: [0.0] * gc_input.shape[0]\n",
        "                    for col in model.get_features()\n",
        "                    if col not in gc_input\n",
        "                }\n",
        "            ),\n",
        "        ],\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    gc_input[pred_col] = model.get_model().predict(gc_input[model.get_features()])\n",
        "\n",
        "    return gc_input[index_cols + [coop_col, pred_col, \"split\"]]\n",
        "\n",
        "\n",
        "def get_gc_predictions(\n",
        "    model,\n",
        "    demand_predictions_path,\n",
        "    date_col,\n",
        "    coop_col,\n",
        "    cat_mapper,\n",
        "    index_cols,\n",
        "    store_level,\n",
        "    corrected_base=True\n",
        "    \n",
        "):\n",
        "\n",
        "    result = pd.merge(\n",
        "        _predict_from_demand(\n",
        "            model,\n",
        "            demand_predictions_path,\n",
        "            date_col,\n",
        "            coop_col,\n",
        "            cat_mapper,\n",
        "            index_cols,\n",
        "            baseline=True,\n",
        "            corrected_base=corrected_base,\n",
        "            store_level=store_level\n",
        "        ),\n",
        "        _predict_from_demand(\n",
        "            model,\n",
        "            demand_predictions_path,\n",
        "            date_col,\n",
        "            coop_col,\n",
        "            cat_mapper,\n",
        "            index_cols,\n",
        "            baseline=False,\n",
        "            corrected_base=corrected_base,\n",
        "            store_level=store_level\n",
        "        ),\n",
        "        on=index_cols + [coop_col, \"split\"],\n",
        "    )\n",
        "\n",
        "    result = result.sort_values(by=[\"coop_name\", \"pos_busn_dt\"], ascending=True)\n",
        "\n",
        "    result[\"pos_busn_dt\"] = pd.to_datetime(result[\"pos_busn_dt\"])\n",
        "    return result\n",
        "\n",
        "\n",
        "def _get_base_gc(\n",
        "    model,\n",
        "    demand_predictions_path,\n",
        "    date_col,\n",
        "    coop_col,\n",
        "    cat_mapper,\n",
        "    index_cols,\n",
        "    store_level,\n",
        "    corrected_base=True\n",
        "    \n",
        "):\n",
        "\n",
        "    base_gc = pd.read_parquet(demand_predictions_path).query('split==\"train\"')\n",
        "\n",
        "    if corrected_base:\n",
        "        base_gc = base_gc.drop(columns=[\"yhatbase\"]).rename(\n",
        "            columns={\"yhatbase_corrected\": \"yhatbase\"}\n",
        "        )\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    base_gc[\"feature_name\"] = base_gc[\"dv_id\"].apply(lambda x: \"item_\" + str(x))\n",
        "    base_gc = base_gc[[date_col, coop_col, \"feature_name\", \"yhatbase\"]].reset_index(\n",
        "        drop=True\n",
        "    )\n",
        "    base_gc[\"estimated_measure\"] = \"gc\"\n",
        "    \n",
        "\n",
        "\n",
        "    base_gc_items = (\n",
        "        base_gc.pipe(\n",
        "            multiindex_pivot,\n",
        "            index=index_cols + [coop_col],\n",
        "            columns=\"feature_name\",\n",
        "            values=\"yhatbase\",\n",
        "        )\n",
        "        .fillna(0)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # base_gc_items = base_gc.pivot(\n",
        "    #     index=index_cols + [coop_col],\n",
        "    #     columns=\"feature_name\",\n",
        "    #     values=\"yhatbase\"\n",
        "    # ).fillna(0).reset_index()\n",
        "\n",
        "    # base_gc_cat = base_gc[index_cols + [coop_col, 'category', 'yhatbase']].groupby(\n",
        "    #     index_cols + [coop_col, 'category'])['yhatbase'].sum().reset_index().pivot(\n",
        "    #     index=index_cols + [coop_col],\n",
        "    #     columns='category',\n",
        "    #     values='yhatbase'\n",
        "    # ).fillna(0).reset_index()\n",
        "\n",
        "    # base_gc = pd.read_parquet(demand_predictions_path).query('split==\"train\"')\n",
        "    # base_gc=scaling_for_category(base_gc,ITEM_MASTER,'yhatbase_corrected')\n",
        "\n",
        "    # if corrected_base:\n",
        "    #     base_gc = base_gc.drop(columns=[\"yhatbase\"]).rename(\n",
        "    #         columns={\"yhatbase_corrected\": \"yhatbase\"}\n",
        "    #     )\n",
        "    # else:\n",
        "    #     pass\n",
        "\n",
        "    # base_gc[\"feature_name\"] = base_gc[\"dv_id\"].apply(lambda x: \"item_\" + str(x))  \n",
        "    # base_gc = base_gc[[date_col, coop_col, \"feature_name\", \"yhatbase\"]].reset_index(\n",
        "    #     drop=True\n",
        "    # )\n",
        "    # base_gc[\"estimated_measure\"] = \"gc\"\n",
        "      \n",
        "    base_gc[\"category\"] = base_gc[\"feature_name\"].map(cat_mapper)\n",
        "\n",
        "    base_gc_cat = (\n",
        "        base_gc[index_cols + [coop_col, \"category\", \"yhatbase\"]]\n",
        "        .groupby(index_cols + [coop_col, \"category\"])[\"yhatbase\"]\n",
        "        .sum()\n",
        "        .reset_index()\n",
        "        .pipe(\n",
        "            multiindex_pivot,\n",
        "            index=index_cols + [coop_col],\n",
        "            columns=\"category\",\n",
        "            values=\"yhatbase\",\n",
        "        )\n",
        "        .fillna(0)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    base_gc = pd.merge(base_gc_items, base_gc_cat, on=index_cols + [coop_col])\n",
        "\n",
        "\n",
        "    base_gc = pd.concat(\n",
        "        [\n",
        "            base_gc,\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    col: [0.0] * base_gc.shape[0]\n",
        "                    for col in model.get_features()\n",
        "                    if col not in base_gc\n",
        "                }\n",
        "            ),\n",
        "        ],\n",
        "        axis=1,\n",
        "    )\n",
        "    \n",
        "\n",
        "    if store_level:\n",
        "      base_gc['str_cnt'] =base_gc['coop_name'].map(str_cnt_mapper)\n",
        "      for col in base_gc.columns:\n",
        "        if col not in ['coop_name','estimated_measure','pos_busn_dt']:\n",
        "\n",
        "          base_gc[col]= base_gc[col]/base_gc['str_cnt']\n",
        "      \n",
        "      base_gc.drop(columns=['str_cnt'],inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    base_gc[\"yhatbase\"] = model.get_model().predict(base_gc[model.get_features()])\n",
        "    return base_gc\n",
        "\n",
        "\n",
        "\n",
        "def simulate_gc_from_predictions(\n",
        "    model,\n",
        "    demand_predictions_path,\n",
        "    demand_tensor_path,\n",
        "    date_col,\n",
        "    coop_col,\n",
        "    index_cols,\n",
        "    cat_mapper,\n",
        "    corrected_base,\n",
        "    store_level,\n",
        "    cold_start=None,\n",
        "    coop=None\n",
        "   \n",
        "):\n",
        "    result = []\n",
        "    logger = set()\n",
        "\n",
        "    # df_demand_tensor = pd.read_csv(demand_tensor_path).query(\n",
        "    #     'dv_id != -1 and not (source==\"cross-learning\" and effect==\"cross\")').reset_index(drop=True).rename(columns={'coop':coop_col}).reset_index(drop=True)\n",
        "    \n",
        "    df_demand_tensor = (\n",
        "        pd.read_csv(demand_tensor_path).query(\"dv_id != -1\").reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    if coop is not None:\n",
        "        df_demand_tensor = df_demand_tensor.query(\"coop_name==@coop\").reset_index(\n",
        "            drop=True\n",
        "        )\n",
        "\n",
        "    df_demand_tensor[\"feature_name\"] = df_demand_tensor[\"dv_id\"].apply(\n",
        "        lambda x: \"item_\" + str(x)\n",
        "    )\n",
        "   \n",
        "\n",
        "    ##get average for november and march \n",
        "    base_gc = _get_base_gc(\n",
        "        model=model,\n",
        "        demand_predictions_path=demand_predictions_path,\n",
        "        date_col=date_col,\n",
        "        coop_col=coop_col,\n",
        "        cat_mapper=cat_mapper,\n",
        "        index_cols=index_cols,\n",
        "        corrected_base=corrected_base,\n",
        "        store_level=store_level\n",
        "    )\n",
        "    \n",
        "    for (coop_name, idv_id, price_point, per_depth), sub_tensor in tqdm(\n",
        "        df_demand_tensor.groupby([\"coop_name\", \"idv_id\", \"price_point\", \"depth_per\"])\n",
        "    ):\n",
        "        scenario_gc = base_gc.copy()\n",
        "\n",
        "        for rec in sub_tensor.to_dict(orient=\"records\"):\n",
        "            if rec[\"feature_name\"] in base_gc.columns:\n",
        "\n",
        "                delta = base_gc[rec[\"feature_name\"]] * (\n",
        "                    rec[\"elasticity\"] * rec[\"depth_per\"]\n",
        "                )\n",
        "                scenario_gc[rec[\"feature_name\"]] += delta\n",
        "                scenario_gc[cat_mapper[rec[\"feature_name\"]]] += delta\n",
        "            else:\n",
        "                logger.add(rec[\"feature_name\"])\n",
        "\n",
        "                continue\n",
        "\n",
        "        scenario_gc[\"yhat\"] = model.get_model().predict(scenario_gc[model.get_features()])\n",
        "        fdf = scenario_gc[scenario_gc[coop_col] == coop_name]\n",
        "        \n",
        "        yhat = fdf[\"yhat\"].mean()\n",
        "        yhatbase = fdf[\"yhatbase\"].mean()\n",
        "        per_gc = (yhat - yhatbase) / yhatbase\n",
        "        elasticity = per_gc / per_depth\n",
        "\n",
        "        result.append(\n",
        "            {\n",
        "                \"yhat\": yhat,\n",
        "                \"yhatbase\": yhatbase,\n",
        "                \"coop_name\": coop_name,\n",
        "                \"idv_id\": idv_id,\n",
        "                \"price_point\": price_point,\n",
        "                \"depth_per\": per_depth,\n",
        "                \"impact\": per_gc,\n",
        "                \"elasticity\": elasticity,\n",
        "            }\n",
        "        )\n",
        "    result = pd.DataFrame(result)\n",
        "    result = pd.merge(\n",
        "        df_demand_tensor[\n",
        "            [\n",
        "                \"coop_name\",\n",
        "                \"idv_id\",\n",
        "                \"daypart\",\n",
        "                \"price_point\",\n",
        "                # 'matching_level',\n",
        "                # 'offer_level_base_price',\n",
        "                \"offer_price\",\n",
        "                \"dis_mech\",\n",
        "                \"item_grp\",\n",
        "            ]\n",
        "        ]\n",
        "        .drop_duplicates()\n",
        "        .reset_index(drop=True),\n",
        "        result,\n",
        "        on=[\"coop_name\", \"idv_id\", \"price_point\"],\n",
        "        how=\"left\",\n",
        "    )\n",
        "    result[\"dv_id\"] = -1\n",
        "    result = (\n",
        "        result.sort_values([\"coop_name\", \"idv_id\", \"price_point\"])\n",
        "        .reset_index(drop=True)\n",
        "        .fillna(0)\n",
        "        .drop_duplicates()\n",
        "    )\n",
        "    if cold_start is not None:\n",
        "        result = pd.concat(\n",
        "            [\n",
        "                pd.read_csv(cold_start)\n",
        "                .query(\"coop_name != @coop\")\n",
        "                .reset_index(drop=True),\n",
        "                result,\n",
        "            ]\n",
        "        )\n",
        "    return result, logger\n",
        "\n",
        "\n",
        "def _simulate_per(model, train_df, yhatbase, feature, category, percentages, coop_col):\n",
        "    result = []\n",
        "    for xper in percentages:\n",
        "        try:\n",
        "            X = train_df[model.get_features()].copy()\n",
        "            delta_counts = xper * X[feature]\n",
        "            X[feature] += delta_counts\n",
        "            X[category] += delta_counts\n",
        "            yper = ((model.get_model().predict(X) - yhatbase) / yhatbase).tolist()\n",
        "            rdf = pd.DataFrame({\n",
        "                coop_col: train_df[coop_col].tolist(),\n",
        "                'per_gc': yper,\n",
        "            })\n",
        "            rdf['feature_name'] = feature\n",
        "            rdf['per_qty'] = xper\n",
        "            gcols = [\"feature_name\", coop_col, \"per_qty\"]\n",
        "            mdf = pd.merge(\n",
        "                rdf.groupby(gcols)[\"per_gc\"].mean().reset_index(),\n",
        "                rdf.groupby(gcols)[\"per_gc\"].count().reset_index().rename(columns={'per_gc': 'count'}),\n",
        "                on=gcols\n",
        "            )\n",
        "            result.append(mdf)\n",
        "        except:\n",
        "            # TODO Get rid of this by fixing missing stuff\n",
        "            pass\n",
        "    return result\n",
        "\n",
        "\n",
        "def simulate_gc_from_actuals(model, gc_item_features_path, gc_category_features_path, demand_tensor_path, index_cols,\n",
        "                             coop_col, response_col, cat_mapper):\n",
        "    train_df, _ = _get_modelling_data(\n",
        "        item_features_path=gc_item_features_path,\n",
        "        category_features_path=gc_category_features_path,\n",
        "        index_cols=index_cols,\n",
        "        coop_col=coop_col,\n",
        "        response_col=response_col\n",
        "    )\n",
        "    df_demand_tensor = pd.read_csv(demand_tensor_path).query('dv_id != -1').reset_index(drop=True)\n",
        "    df_demand_tensor['feature_name'] = df_demand_tensor['dv_id'].apply(lambda x: \"item_\" + str(x))\n",
        "    #percentages = df_demand_tensor.groupby(['feature_name'])['impact'].apply(lambda x: list(set(x))).to_dict()\n",
        "\n",
        "    print(percentages)\n",
        "    yhatbase = model.get_model().predict(train_df[model.get_features()])\n",
        "    result = []\n",
        "    for feature in tqdm(model.get_features()):\n",
        "        if feature in cat_mapper and feature in percentages:\n",
        "            category = cat_mapper[feature]\n",
        "            result += _simulate_per(model, train_df, yhatbase, feature, category, percentages[feature], coop_col)\n",
        "    result = pd.concat(result, ignore_index=True)\n",
        "    result['elasticity'] = result['per_gc'] / result['per_qty']\n",
        "    result = pd.merge(\n",
        "        df_demand_tensor.rename(columns={'caliberated_elas': 'demand_elasticity','coop':'coop_name'}),\n",
        "        result.drop(columns=['per_gc', 'count']).rename(\n",
        "            columns={'per_qty': 'impact', 'elasticity': 'gc_elasticity_item'}),\n",
        "        on=['feature_name', 'coop_name', 'impact'],\n",
        "        how='left'\n",
        "    )\n",
        "    result['elasticity'] = result['gc_elasticity_item'] * result['demand_elasticity']\n",
        "    result = result.groupby([\n",
        "        'coop_name',\n",
        "        'idv_id',\n",
        "        'daypart',\n",
        "        'price_point',\n",
        "        'base_price',\n",
        "        'dis_mech',\n",
        "        'item_grp',\n",
        "        'depth_per',\n",
        "    ])['elasticity'].sum().reset_index()\n",
        "    result['impact'] = result['elasticity'] * result['depth_per']\n",
        "    return result.sort_values(['coop_name', 'idv_id', 'price_point']).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "976esfqhFzUv"
      },
      "source": [
        "#### Training/Loading Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_refit_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPzVR9ptXXu5",
        "outputId": "c3823681-451f-4e14-c44e-93a639c5e22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_ITERATION_ROOT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NCXKmIDYVXrM",
        "outputId": "5286327b-14b7-40e3-fe21-12bbe025263a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/03_gc_models/xgb_models/18'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp_gc_item_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SxGR6ladVZvl",
        "outputId": "2463ef01-7e85-4091-a4bd-37e374d46e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X Item X Categories/14/items'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp_gc_category_features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Qhj5e4y-k6bQ",
        "outputId": "da8ea8fd-c728-4dc3-b682-4a1710a6271f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X Item X Categories/14/categories'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWKzbFrV-hYx",
        "outputId": "119f8d72-f77c-4ff3-b4fd-cc2abcd28cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.2 ms, sys: 529 ms, total: 542 ms\n",
            "Wall time: 1.56 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "if cfg_refit_model:\n",
        "  xgb_model = fit_model(\n",
        "          gc_item_features_path=fp_gc_item_features,\n",
        "          gc_category_features_path=fp_gc_category_features,\n",
        "          index_cols=cl_index_cols,\n",
        "          coop_col=cl_coop_col,\n",
        "          response_col=cl_response_col,\n",
        "          grid_filepath=fp_grid,\n",
        "          xgb_num_tasks=cfg_xgb_num_tasks,\n",
        "          xgb_num_threads_per_task=cfg_xgb_num_threads_per_task,\n",
        "          iteration_root=MODEL_ITERATION_ROOT\n",
        "      )\n",
        "else:\n",
        "    xgb_model = GBDTRegressionModel(\n",
        "        self_features=[],\n",
        "        control_features=[],\n",
        "        subst_features=[],\n",
        "        compl_features=[],\n",
        "        no_offer_level=0\n",
        "    ).load_model(\n",
        "        os.path.join(MODEL_ITERATION_ROOT, \"model.pkl\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = GBDTRegressionModel(\n",
        "        self_features=[],\n",
        "        control_features=[],\n",
        "        subst_features=[],\n",
        "        compl_features=[],\n",
        "        no_offer_level=0\n",
        "    )\n",
        "xgb_model._estimator = xgb.Booster()\n",
        "xgb_model._estimator.load_model(path_to_ubj)\n",
        "xgb_model._self_features = xgb_model._feature_names"
      ],
      "metadata": {
        "id": "tMA13nKaOoSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_gc_item_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0UQnCB5ugz1S",
        "outputId": "d2950044-3624-455a-805d-570f6c981cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X Item X Categories/14/items'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp_gc_category_features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uoc6b1fNg04u",
        "outputId": "c3441bfb-6c26-4c5b-be41-867773837fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/01_data/y_data/02_gc_day_level/Coop X Item X Categories/14/categories'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MODEL_ITERATION_ROOT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "je0PSZUh3Ex8",
        "outputId": "1f2fe8e2-59c3-45c2-f4f7-df23da9fe2d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/03_gc_models/xgb_models/18'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp_demand_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aciXGKhGBFz7",
        "outputId": "4f192a8c-b44e-457a-b7dd-04be30528564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/41/01_demand/model_collated/model_predictions.parquet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tensor_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jfltkvkIBG97",
        "outputId": "e8b1aba7-75e3-417f-e115-5717c682fe8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/41/04_tensor/tensor_processed/all_tensor_processed_raw.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate GC Elasticity and Predictions "
      ],
      "metadata": {
        "id": "fQf9uzOa6x5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result, lgr = simulate_gc_from_predictions(\n",
        "      model=xgb_model,\n",
        "      demand_predictions_path=fp_demand_predictions,\n",
        "      demand_tensor_path=output_tensor_path,\n",
        "      date_col=\"pos_busn_dt\",\n",
        "      coop_col=\"coop_name\",\n",
        "      index_cols=[\"pos_busn_dt\", \"estimated_measure\"],\n",
        "      cat_mapper=dict_cat_mapper,\n",
        "      corrected_base=True,\n",
        "      store_level=False\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGLczZmh3Azx",
        "outputId": "b04918e3-76d1-4e17-dd03-567e313520d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8590/8590 [05:31<00:00, 25.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_gc_simulation_filepath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3R2WfGcdoTtW",
        "outputId": "d80102c0-f190-41cc-b089-5f83c4ec8cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/41/06_gc_xgb/gc_simulation_method_1.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output_gc_simulation_filepath.replace('gc_simulation_method_1.csv','14/gc_simulation_method_1.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bM4ejH8xwSkB",
        "outputId": "31108be4-0482-4399-d004-bf0908764ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/31/06_gc_xgb/14/gc_simulation_method_1.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output_gc_simulation_filepath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "irgOD9iEc3fw",
        "outputId": "44662d98-68a0-40f4-acd5-0d35222ad811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/33/06_gc_xgb/gc_simulation_method_1.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv(output_gc_simulation_filepath,index=False)"
      ],
      "metadata": {
        "id": "yPPPDBoBoQa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_level=True"
      ],
      "metadata": {
        "id": "NA7d1RyY_SNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_gc_simulation_filepath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4yLDe1pY_pUQ",
        "outputId": "797d9c3a-23cc-4ca6-ed10-c0701a7a8536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/41/06_gc_xgb/gc_simulation_method_1.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp_7 ='/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/25/06_gc_xgb/exp-7/gc_simulation_method_1.csv'"
      ],
      "metadata": {
        "id": "x_WUSSN_Cg3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_8 ='/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/25/06_gc_xgb/exp-8/gc_simulation_method_1.csv'"
      ],
      "metadata": {
        "id": "wh9iDIjyVgdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#result.to_csv(exp_8, index=None)\n"
      ],
      "metadata": {
        "id": "d0qUNSPO8WL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_demand_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hVoaRQhsflEz",
        "outputId": "19d46cf8-2b77-4396-d26c-cff619ec5857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/33/01_demand/model_collated/model_predictions.parquet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ITERATION_ROOT\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mAs0WSbGbtG-",
        "outputId": "d1d93ec0-f10a-4f51-c685-47df81dddd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/03_gc_models/xgb_models/16'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "\n",
        "# if save_gc_elasticitity:\n",
        "#   result.to_csv(output_gc_predictions_filepath, index=None)\n",
        "\n",
        "gc_predictions = get_gc_predictions(\n",
        "      model=xgb_model,\n",
        "      demand_predictions_path=fp_demand_predictions,\n",
        "      date_col=\"pos_busn_dt\",\n",
        "      coop_col=\"coop_name\",\n",
        "      cat_mapper=dict_cat_mapper,\n",
        "      index_cols=[\"pos_busn_dt\", \"estimated_measure\"],\n",
        "      corrected_base=True,\n",
        "      store_level=False)\n",
        "\n",
        "# if save_gc_predictions:\n",
        "#   gc_predictions.to_csv(output_gc_predictions_filepath, index=None)"
      ],
      "metadata": {
        "id": "_tTTKYtD_vrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_gc_predictions_filepath.replace('model_predictions.csv','14/model_predictions.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ooeOpCiF9soz",
        "outputId": "fd32cc7a-6a6d-4eab-ba93-e4079e4dfeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/31/06_gc_xgb/14/model_predictions.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_gc_predictions_filepath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KOEdBmDWBky6",
        "outputId": "ea4cbf61-e3d9-4dd7-bbf1-826062971f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/41/06_gc_xgb/model_predictions.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predictions.to_csv(output_gc_predictions_filepath,index=False)"
      ],
      "metadata": {
        "id": "g_M7mVYNKiMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create GC DEBUG Notebook"
      ],
      "metadata": {
        "id": "mnu8PPbp9Afu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_gc =pd.read_parquet(GC_DATA_PATH_WEEKLY.replace('coop=','')).sort_values(['coop','pos_busn_dt']).rename(columns={'total_gc':'actual_gc','coop':'coop_name'})\n",
        "actual_gc['pos_busn_dt']=pd.to_datetime(actual_gc['pos_busn_dt'])\n",
        "# gc_predicted_merge = gc_predictions.merge(actual_gc,on=['coop_name','pos_busn_dt'],how='left')\n",
        "# gc_predicted_merge['store_count'] =gc_predicted_merge['coop_name'].map(str_cnt_mapper)\n",
        "# gc_predicted_merge['yhat_per_store'] = gc_predicted_merge['yhat']/ gc_predicted_merge['store_count']\n",
        "# gc_predicted_merge['yhatbase_per_store'] =  gc_predicted_merge['yhatbase']/ gc_predicted_merge['store_count']\n",
        "# gc_predicted_merge['actual_gc_per_store'] = gc_predicted_merge['actual_gc']/ gc_predicted_merge['store_count']"
      ],
      "metadata": {
        "id": "tQH0YEWX5rw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predictions['str_count'] =gc_predictions['coop_name'].map(str_cnt_mapper)"
      ],
      "metadata": {
        "id": "lAgOnWNj6oob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predictions = gc_predictions.merge(actual_gc,on=['coop_name','pos_busn_dt'],how='left')"
      ],
      "metadata": {
        "id": "IT1H4F7QOEUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_gc_predictions_filepath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WRATTF6tfVnX",
        "outputId": "b9eaa539-441a-44c0-b512-0408d239ef4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/26/06_gc_xgb/model_predictions.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predictions.to_csv(\"/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/26/06_gc_xgb/model_predictions_decorr.csv\",index=False)"
      ],
      "metadata": {
        "id": "ngo7lYgbf-he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5welfVygB91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gc_predictions.to_csv(exp_7.replace('gc_simulation_method_1.csv','model_predictions_7.csv'),index=False)"
      ],
      "metadata": {
        "id": "njJq-7D-M6ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4XTdIDbMOQ2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predictions['yhatbase_corrected_coop'] = gc_predictions['yhatbase_corrected']*gc_predictions['str_count']\n",
        "gc_predictions['yhat_coop']= gc_predictions['yhat']*gc_predictions['str_count']\n"
      ],
      "metadata": {
        "id": "Z_yq-_52cPHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predictions.to_csv(output_gc_predictions_filepath.replace('/model_predictions.csv','/per_store/model_predictions.csv'),index=False)"
      ],
      "metadata": {
        "id": "5TO5DN5wl5l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predicted_merge = gc_predictions.merge(actual_gc,on=['coop_name','pos_busn_dt'],how='left')"
      ],
      "metadata": {
        "id": "vRULJNG5cfQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predicted_merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "M3IyihXAl42V",
        "outputId": "05dd4abe-e316-4de5-94f9-2f7e037f37a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     pos_busn_dt estimated_measure                              coop_name  \\\n",
              "0     2019-01-07                gc                    albuquerque el paso   \n",
              "1     2019-01-14                gc                    albuquerque el paso   \n",
              "2     2019-01-21                gc                    albuquerque el paso   \n",
              "3     2019-01-28                gc                    albuquerque el paso   \n",
              "4     2019-02-04                gc                    albuquerque el paso   \n",
              "...          ...               ...                                    ...   \n",
              "2404  2023-07-03                gc  washington dc baltimore eastern shore   \n",
              "2405  2023-07-10                gc  washington dc baltimore eastern shore   \n",
              "2406  2023-07-17                gc  washington dc baltimore eastern shore   \n",
              "2407  2023-07-24                gc  washington dc baltimore eastern shore   \n",
              "2408  2023-07-31                gc  washington dc baltimore eastern shore   \n",
              "\n",
              "      yhatbase_corrected  split         yhat  str_count  \\\n",
              "0            7237.396484  train  7348.896484      132.0   \n",
              "1            7148.249512  train  7230.482910      132.0   \n",
              "2            7085.852539  train  7162.291992      132.0   \n",
              "3            7234.362793  train  7238.036133      132.0   \n",
              "4            7520.579102  train  7561.579590      132.0   \n",
              "...                  ...    ...          ...        ...   \n",
              "2404         8177.697266   test  8177.697266      460.0   \n",
              "2405         8171.047363   test  8171.047363      460.0   \n",
              "2406         8159.281738   test  8159.281738      460.0   \n",
              "2407         8236.229492   test  8236.229492      460.0   \n",
              "2408         8244.052734   test  8244.052734      460.0   \n",
              "\n",
              "      yhatbase_corrected_coop     yhat_coop  actual_gc  \n",
              "0                9.553363e+05  9.700543e+05   961324.0  \n",
              "1                9.435689e+05  9.544237e+05   962712.0  \n",
              "2                9.353325e+05  9.454225e+05   952774.0  \n",
              "3                9.549359e+05  9.554208e+05   988217.0  \n",
              "4                9.927164e+05  9.981285e+05   996475.0  \n",
              "...                       ...           ...        ...  \n",
              "2404             3.761741e+06  3.761741e+06        NaN  \n",
              "2405             3.758682e+06  3.758682e+06        NaN  \n",
              "2406             3.753270e+06  3.753270e+06        NaN  \n",
              "2407             3.788666e+06  3.788666e+06        NaN  \n",
              "2408             3.792264e+06  3.792264e+06        NaN  \n",
              "\n",
              "[2409 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72283103-aa03-42f6-b41a-dfa6e1c8ee1f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos_busn_dt</th>\n",
              "      <th>estimated_measure</th>\n",
              "      <th>coop_name</th>\n",
              "      <th>yhatbase_corrected</th>\n",
              "      <th>split</th>\n",
              "      <th>yhat</th>\n",
              "      <th>str_count</th>\n",
              "      <th>yhatbase_corrected_coop</th>\n",
              "      <th>yhat_coop</th>\n",
              "      <th>actual_gc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-01-07</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7237.396484</td>\n",
              "      <td>train</td>\n",
              "      <td>7348.896484</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.553363e+05</td>\n",
              "      <td>9.700543e+05</td>\n",
              "      <td>961324.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-01-14</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7148.249512</td>\n",
              "      <td>train</td>\n",
              "      <td>7230.482910</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.435689e+05</td>\n",
              "      <td>9.544237e+05</td>\n",
              "      <td>962712.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-21</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7085.852539</td>\n",
              "      <td>train</td>\n",
              "      <td>7162.291992</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.353325e+05</td>\n",
              "      <td>9.454225e+05</td>\n",
              "      <td>952774.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-01-28</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7234.362793</td>\n",
              "      <td>train</td>\n",
              "      <td>7238.036133</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.549359e+05</td>\n",
              "      <td>9.554208e+05</td>\n",
              "      <td>988217.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-02-04</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7520.579102</td>\n",
              "      <td>train</td>\n",
              "      <td>7561.579590</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.927164e+05</td>\n",
              "      <td>9.981285e+05</td>\n",
              "      <td>996475.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>2023-07-03</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8177.697266</td>\n",
              "      <td>test</td>\n",
              "      <td>8177.697266</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.761741e+06</td>\n",
              "      <td>3.761741e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2405</th>\n",
              "      <td>2023-07-10</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8171.047363</td>\n",
              "      <td>test</td>\n",
              "      <td>8171.047363</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.758682e+06</td>\n",
              "      <td>3.758682e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2406</th>\n",
              "      <td>2023-07-17</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8159.281738</td>\n",
              "      <td>test</td>\n",
              "      <td>8159.281738</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.753270e+06</td>\n",
              "      <td>3.753270e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2407</th>\n",
              "      <td>2023-07-24</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8236.229492</td>\n",
              "      <td>test</td>\n",
              "      <td>8236.229492</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.788666e+06</td>\n",
              "      <td>3.788666e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2408</th>\n",
              "      <td>2023-07-31</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8244.052734</td>\n",
              "      <td>test</td>\n",
              "      <td>8244.052734</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.792264e+06</td>\n",
              "      <td>3.792264e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2409 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72283103-aa03-42f6-b41a-dfa6e1c8ee1f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72283103-aa03-42f6-b41a-dfa6e1c8ee1f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72283103-aa03-42f6-b41a-dfa6e1c8ee1f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc_predicted_merge.to_csv(output_gc_predictions_filepath.replace('/model_predictions.csv','/per_store/model_predictions_debug.csv'),index=False)"
      ],
      "metadata": {
        "id": "CGSMMmDNcitA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mapes(df,y1,y2):\n",
        "  metrics = df.groupby('coop_name').apply(lambda x: wmape(x[y1], x[y2])).reset_index().rename(\n",
        "           columns={0: 'wmape_train'})\n",
        "  return metrics    "
      ],
      "metadata": {
        "id": "tkWpyw4_mF2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### comparison"
      ],
      "metadata": {
        "id": "nzpwrqBDetH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#old_predictions =pd.read_csv(output_gc_predictions_filepath)\n",
        "#old_predictions['pos_busn_dt']=pd.to_datetime(old_predictions['pos_busn_dt'])"
      ],
      "metadata": {
        "id": "KTKXAJPRd7Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_predictions = old_predictions.merge(actual_gc,on=['coop_name','pos_busn_dt'],how='left')"
      ],
      "metadata": {
        "id": "vLUjqUVEmg6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = get_mapes(old_predictions,'yhatbase_corrected','actual_gc')"
      ],
      "metadata": {
        "id": "zunL-hPtfGqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics.to_csv(output_gc_predictions_filepath.replace('model_predictions.csv','model_metrics.csv'),index=False)"
      ],
      "metadata": {
        "id": "SzEoNvI4mwpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_predictions = pd.read_csv(output_gc_predictions_filepath.replace('/model_predictions.csv','/per_store/model_predictions_debug.csv'))\n",
        "new_predictions['pos_busn_dt']=pd.to_datetime(new_predictions['pos_busn_dt'])"
      ],
      "metadata": {
        "id": "HPiIOgWoe1ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "-KnaSdMpnFQT",
        "outputId": "4ea4ae1d-4d08-498e-ec30-d39360aec1c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     pos_busn_dt estimated_measure                              coop_name  \\\n",
              "0     2019-01-07                gc                    albuquerque el paso   \n",
              "1     2019-01-14                gc                    albuquerque el paso   \n",
              "2     2019-01-21                gc                    albuquerque el paso   \n",
              "3     2019-01-28                gc                    albuquerque el paso   \n",
              "4     2019-02-04                gc                    albuquerque el paso   \n",
              "...          ...               ...                                    ...   \n",
              "2404  2023-07-03                gc  washington dc baltimore eastern shore   \n",
              "2405  2023-07-10                gc  washington dc baltimore eastern shore   \n",
              "2406  2023-07-17                gc  washington dc baltimore eastern shore   \n",
              "2407  2023-07-24                gc  washington dc baltimore eastern shore   \n",
              "2408  2023-07-31                gc  washington dc baltimore eastern shore   \n",
              "\n",
              "      yhatbase_corrected  split       yhat  str_count  \\\n",
              "0              7237.3965  train  7348.8965      132.0   \n",
              "1              7148.2495  train  7230.4830      132.0   \n",
              "2              7085.8525  train  7162.2920      132.0   \n",
              "3              7234.3630  train  7238.0360      132.0   \n",
              "4              7520.5790  train  7561.5796      132.0   \n",
              "...                  ...    ...        ...        ...   \n",
              "2404           8177.6973   test  8177.6973      460.0   \n",
              "2405           8171.0474   test  8171.0474      460.0   \n",
              "2406           8159.2817   test  8159.2817      460.0   \n",
              "2407           8236.2295   test  8236.2295      460.0   \n",
              "2408           8244.0530   test  8244.0530      460.0   \n",
              "\n",
              "      yhatbase_corrected_coop     yhat_coop  actual_gc  \n",
              "0                9.553363e+05  9.700543e+05   961324.0  \n",
              "1                9.435689e+05  9.544237e+05   962712.0  \n",
              "2                9.353325e+05  9.454225e+05   952774.0  \n",
              "3                9.549359e+05  9.554208e+05   988217.0  \n",
              "4                9.927164e+05  9.981285e+05   996475.0  \n",
              "...                       ...           ...        ...  \n",
              "2404             3.761741e+06  3.761741e+06        NaN  \n",
              "2405             3.758682e+06  3.758682e+06        NaN  \n",
              "2406             3.753270e+06  3.753270e+06        NaN  \n",
              "2407             3.788666e+06  3.788666e+06        NaN  \n",
              "2408             3.792264e+06  3.792264e+06        NaN  \n",
              "\n",
              "[2409 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-393a1dfb-066b-40ed-9d20-1bf43d46fa0a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pos_busn_dt</th>\n",
              "      <th>estimated_measure</th>\n",
              "      <th>coop_name</th>\n",
              "      <th>yhatbase_corrected</th>\n",
              "      <th>split</th>\n",
              "      <th>yhat</th>\n",
              "      <th>str_count</th>\n",
              "      <th>yhatbase_corrected_coop</th>\n",
              "      <th>yhat_coop</th>\n",
              "      <th>actual_gc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-01-07</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7237.3965</td>\n",
              "      <td>train</td>\n",
              "      <td>7348.8965</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.553363e+05</td>\n",
              "      <td>9.700543e+05</td>\n",
              "      <td>961324.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-01-14</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7148.2495</td>\n",
              "      <td>train</td>\n",
              "      <td>7230.4830</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.435689e+05</td>\n",
              "      <td>9.544237e+05</td>\n",
              "      <td>962712.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-01-21</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7085.8525</td>\n",
              "      <td>train</td>\n",
              "      <td>7162.2920</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.353325e+05</td>\n",
              "      <td>9.454225e+05</td>\n",
              "      <td>952774.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-01-28</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7234.3630</td>\n",
              "      <td>train</td>\n",
              "      <td>7238.0360</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.549359e+05</td>\n",
              "      <td>9.554208e+05</td>\n",
              "      <td>988217.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-02-04</td>\n",
              "      <td>gc</td>\n",
              "      <td>albuquerque el paso</td>\n",
              "      <td>7520.5790</td>\n",
              "      <td>train</td>\n",
              "      <td>7561.5796</td>\n",
              "      <td>132.0</td>\n",
              "      <td>9.927164e+05</td>\n",
              "      <td>9.981285e+05</td>\n",
              "      <td>996475.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>2023-07-03</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8177.6973</td>\n",
              "      <td>test</td>\n",
              "      <td>8177.6973</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.761741e+06</td>\n",
              "      <td>3.761741e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2405</th>\n",
              "      <td>2023-07-10</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8171.0474</td>\n",
              "      <td>test</td>\n",
              "      <td>8171.0474</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.758682e+06</td>\n",
              "      <td>3.758682e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2406</th>\n",
              "      <td>2023-07-17</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8159.2817</td>\n",
              "      <td>test</td>\n",
              "      <td>8159.2817</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.753270e+06</td>\n",
              "      <td>3.753270e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2407</th>\n",
              "      <td>2023-07-24</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8236.2295</td>\n",
              "      <td>test</td>\n",
              "      <td>8236.2295</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.788666e+06</td>\n",
              "      <td>3.788666e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2408</th>\n",
              "      <td>2023-07-31</td>\n",
              "      <td>gc</td>\n",
              "      <td>washington dc baltimore eastern shore</td>\n",
              "      <td>8244.0530</td>\n",
              "      <td>test</td>\n",
              "      <td>8244.0530</td>\n",
              "      <td>460.0</td>\n",
              "      <td>3.792264e+06</td>\n",
              "      <td>3.792264e+06</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2409 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-393a1dfb-066b-40ed-9d20-1bf43d46fa0a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-393a1dfb-066b-40ed-9d20-1bf43d46fa0a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-393a1dfb-066b-40ed-9d20-1bf43d46fa0a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_new = get_mapes(new_predictions,'yhat_coop','actual_gc')"
      ],
      "metadata": {
        "id": "aQRuwyyfgoZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics_new.to_csv(output_gc_predictions_filepath.replace('/model_predictions.csv','/per_store/model_metrics.csv'),index=False)"
      ],
      "metadata": {
        "id": "9XpFMTDonKhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Normalized Categories"
      ],
      "metadata": {
        "id": "4nwRyhYOdqtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp_demand_predictions"
      ],
      "metadata": {
        "id": "OgMOb5fRSf9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "dd75e216-91cd-4e3c-cbcb-0464986206c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shareddrives/MCD_Discount Engine/US/2022/4. Modeling/99_pipeline/31/01_demand/model_collated/model_predictions.parquet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_demand_predictions.query('coop_name==\"denver col springs s colorado\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "kb3cKZ8Biqs6",
        "outputId": "cacab807-2e62-489f-d8e3-13b7353bec94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      estimated_measure  dv_id pos_busn_dt           y        yhat  \\\n",
              "0              item_qty      1  2019-01-07  222.151220  259.498349   \n",
              "1              item_qty      1  2019-01-14  228.931707  259.109399   \n",
              "2              item_qty      1  2019-01-21  220.960976  258.960507   \n",
              "3              item_qty      1  2019-01-28  247.639024  278.078261   \n",
              "4              item_qty      1  2019-02-04  235.190244  274.750221   \n",
              "...                 ...    ...         ...         ...         ...   \n",
              "35467              None   9842  2024-01-29    0.000000  138.599736   \n",
              "35468              None   9842  2024-02-05    0.000000  140.877880   \n",
              "35469              None   9842  2024-02-12    0.000000  140.600294   \n",
              "35470              None   9842  2024-02-19    0.000000  141.438925   \n",
              "35471              None   9842  2024-02-26    0.000000  143.015276   \n",
              "\n",
              "         yhatbase   algo  split                      coop_name  \\\n",
              "0      279.066332  lasso  train  denver col springs s colorado   \n",
              "1      278.677381  lasso  train  denver col springs s colorado   \n",
              "2      278.528489  lasso  train  denver col springs s colorado   \n",
              "3      278.078261  lasso  train  denver col springs s colorado   \n",
              "4      274.750221  lasso  train  denver col springs s colorado   \n",
              "...           ...    ...    ...                            ...   \n",
              "35467  138.599736  lasso   test  denver col springs s colorado   \n",
              "35468  140.877880  lasso   test  denver col springs s colorado   \n",
              "35469  140.600294  lasso   test  denver col springs s colorado   \n",
              "35470  141.438925  lasso   test  denver col springs s colorado   \n",
              "35471  143.015276  lasso   test  denver col springs s colorado   \n",
              "\n",
              "       yhatbase_corrected    item_category  \n",
              "0              279.066332          ENTREES  \n",
              "1              278.677381          ENTREES  \n",
              "2              278.528489          ENTREES  \n",
              "3              278.078261          ENTREES  \n",
              "4              274.750221          ENTREES  \n",
              "...                   ...              ...  \n",
              "35467          138.599736  BREAKFAST MEALS  \n",
              "35468          140.877880  BREAKFAST MEALS  \n",
              "35469          140.600294  BREAKFAST MEALS  \n",
              "35470          141.438925  BREAKFAST MEALS  \n",
              "35471          143.015276  BREAKFAST MEALS  \n",
              "\n",
              "[35472 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-976c1ac1-d689-4cbf-b84d-82142c5178d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>estimated_measure</th>\n",
              "      <th>dv_id</th>\n",
              "      <th>pos_busn_dt</th>\n",
              "      <th>y</th>\n",
              "      <th>yhat</th>\n",
              "      <th>yhatbase</th>\n",
              "      <th>algo</th>\n",
              "      <th>split</th>\n",
              "      <th>coop_name</th>\n",
              "      <th>yhatbase_corrected</th>\n",
              "      <th>item_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>item_qty</td>\n",
              "      <td>1</td>\n",
              "      <td>2019-01-07</td>\n",
              "      <td>222.151220</td>\n",
              "      <td>259.498349</td>\n",
              "      <td>279.066332</td>\n",
              "      <td>lasso</td>\n",
              "      <td>train</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>279.066332</td>\n",
              "      <td>ENTREES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>item_qty</td>\n",
              "      <td>1</td>\n",
              "      <td>2019-01-14</td>\n",
              "      <td>228.931707</td>\n",
              "      <td>259.109399</td>\n",
              "      <td>278.677381</td>\n",
              "      <td>lasso</td>\n",
              "      <td>train</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>278.677381</td>\n",
              "      <td>ENTREES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>item_qty</td>\n",
              "      <td>1</td>\n",
              "      <td>2019-01-21</td>\n",
              "      <td>220.960976</td>\n",
              "      <td>258.960507</td>\n",
              "      <td>278.528489</td>\n",
              "      <td>lasso</td>\n",
              "      <td>train</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>278.528489</td>\n",
              "      <td>ENTREES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>item_qty</td>\n",
              "      <td>1</td>\n",
              "      <td>2019-01-28</td>\n",
              "      <td>247.639024</td>\n",
              "      <td>278.078261</td>\n",
              "      <td>278.078261</td>\n",
              "      <td>lasso</td>\n",
              "      <td>train</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>278.078261</td>\n",
              "      <td>ENTREES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>item_qty</td>\n",
              "      <td>1</td>\n",
              "      <td>2019-02-04</td>\n",
              "      <td>235.190244</td>\n",
              "      <td>274.750221</td>\n",
              "      <td>274.750221</td>\n",
              "      <td>lasso</td>\n",
              "      <td>train</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>274.750221</td>\n",
              "      <td>ENTREES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35467</th>\n",
              "      <td>None</td>\n",
              "      <td>9842</td>\n",
              "      <td>2024-01-29</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>138.599736</td>\n",
              "      <td>138.599736</td>\n",
              "      <td>lasso</td>\n",
              "      <td>test</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>138.599736</td>\n",
              "      <td>BREAKFAST MEALS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35468</th>\n",
              "      <td>None</td>\n",
              "      <td>9842</td>\n",
              "      <td>2024-02-05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>140.877880</td>\n",
              "      <td>140.877880</td>\n",
              "      <td>lasso</td>\n",
              "      <td>test</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>140.877880</td>\n",
              "      <td>BREAKFAST MEALS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35469</th>\n",
              "      <td>None</td>\n",
              "      <td>9842</td>\n",
              "      <td>2024-02-12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>140.600294</td>\n",
              "      <td>140.600294</td>\n",
              "      <td>lasso</td>\n",
              "      <td>test</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>140.600294</td>\n",
              "      <td>BREAKFAST MEALS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35470</th>\n",
              "      <td>None</td>\n",
              "      <td>9842</td>\n",
              "      <td>2024-02-19</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>141.438925</td>\n",
              "      <td>141.438925</td>\n",
              "      <td>lasso</td>\n",
              "      <td>test</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>141.438925</td>\n",
              "      <td>BREAKFAST MEALS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35471</th>\n",
              "      <td>None</td>\n",
              "      <td>9842</td>\n",
              "      <td>2024-02-26</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>143.015276</td>\n",
              "      <td>143.015276</td>\n",
              "      <td>lasso</td>\n",
              "      <td>test</td>\n",
              "      <td>denver col springs s colorado</td>\n",
              "      <td>143.015276</td>\n",
              "      <td>BREAKFAST MEALS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35472 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-976c1ac1-d689-4cbf-b84d-82142c5178d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-976c1ac1-d689-4cbf-b84d-82142c5178d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-976c1ac1-d689-4cbf-b84d-82142c5178d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "A = ['Apple', 'Dog']\n",
        "B = ['Cat', 'OWL', 'PEACOCK']\n",
        "df1 = pd.DataFrame({'A':A})\n",
        "df2 = pd.DataFrame({'B':B})\n",
        "pd.concat([df1,df2],axis=1).to_csv('myfile.csv', index = False)"
      ],
      "metadata": {
        "id": "MBt3-Frketrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5KaYStEsBL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}